{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2a71f",
   "metadata": {},
   "source": [
    "# Local Data Tutorial\n",
    "\n",
    "In this tutorial, rather than running real models and configurations over MIMIC-IV, we'll work with a set of\n",
    "local, synthetic files distributed with this repository, with the goal being to fully explore the details of\n",
    "this pipeline. This tutorial will consist of both content on this page, running certain scripts on one's local\n",
    "machine, and some jupyter notebooks. We will walk through the entire pipeline with these local examples and\n",
    "discuss limitations of the pipeline, details of classes, scripts, etc.\n",
    "\n",
    "## Synthetic Data\n",
    "\n",
    "For this tutorial, we'll use the three synthetic data files distributed in the [sample_data/raw](<>) folder in\n",
    "the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b07a006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 50M\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 12:42 \u001b[0m\u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 5 mmd mmd 4.0K Jul 16 16:20 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 3.6M Jul 14 17:13 admit_vitals.csv\r\n",
      "-rw-rw-r-- 1 mmd mmd  47M Jul 14 17:13 labs.csv\r\n",
      "-rw-rw-r-- 1 mmd mmd 4.2K Jul 14 17:13 subjects.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls --color -lah raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41de5b",
   "metadata": {},
   "source": [
    "```{note}\n",
    "To see how those files are generated, look at [sample_data/generate_synthetic_data.ipynb]().\n",
    "```\n",
    "\n",
    "These files contain the following data:\n",
    "\n",
    "### `subjects.csv`\n",
    "\n",
    "This file contains per-subject data. It has one row per subject, with each row containing a subject identifier (here called \"`MRN`\"), a date of birth (\"`dob`\"), the subject's eye color (`eye_color`), and the subject's height (\"`height`\"):\n",
    "\n",
    "```{note}\n",
    "To examine these csv files locally, we use the `csvkit` unix command line package. You can learn more about it [here](https://csvkit.readthedocs.io/en/latest/index.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a99ad356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     MRN |        dob | eye_color |   height |\r\n",
      "| ------- | ---------- | --------- | -------- |\r\n",
      "| 310,243 | 1981-07-28 | GREEN     | 178.768… |\r\n",
      "| 384,198 | 1985-04-15 | BROWN     | 168.319… |\r\n",
      "| 520,533 | 1979-04-15 | BROWN     | 165.836… |\r\n",
      "| 850,710 | 1970-08-08 | HAZEL     | 159.722… |\r\n",
      "| 379,433 | 1971-06-25 | BLUE      | 153.749… |\r\n",
      "|     ... |        ... | ...       |      ... |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" --max-rows=5 raw/subjects.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863c2c8",
   "metadata": {},
   "source": [
    "### `admit_vitals.csv`\n",
    "\n",
    "This file contains dynamic data quantifying both fictional subject hospital admissions, and fictional vitals signs measured for those subjects. Each row of this file records a unique vitals sign measurement for a patient, affiliated with the associated admission listed in the row. This means that admission level information is _heavily duplicated_ within this file, which is a phenomena sometimes observed in real data, and something we'll need to account for in our pipeline's setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7878a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       MRN | admit_date           | disch_date           | department | vitals_date          |     HR |  temp |\r\n",
      "| --------- | -------------------- | -------------------- | ---------- | -------------------- | ------ | ----- |\r\n",
      "| 1,549,363 | 01/04/2010, 06:36:31 | 01/14/2010, 11:41:31 | ORTHOPEDIC | 01/11/2010, 14:18:35 |   42.7 |  94.6 |\r\n",
      "|   415,881 | 02/11/2010, 04:59:07 | 02/14/2010, 07:56:23 | ORTHOPEDIC | 02/11/2010, 10:34:17 |  148.5 |  95.2 |\r\n",
      "|    42,335 | 03/06/2010, 05:33:18 | 03/16/2010, 05:09:11 | CARDIAC    | 03/13/2010, 10:47:39 | -134.8 | 101.0 |\r\n",
      "| 1,516,810 | 02/11/2010, 23:23:26 | 02/22/2010, 23:59:04 | CARDIAC    | 02/12/2010, 16:58:44 |   94.2 |  94.8 |\r\n",
      "|   928,262 | 02/19/2010, 03:51:12 | 02/20/2010, 23:32:31 | CARDIAC    | 02/19/2010, 18:08:30 |   77.3 |  99.7 |\r\n",
      "|       ... | ...                  | ...                  | ...        | ...                  |    ... |   ... |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" --max-rows=5 raw/admit_vitals.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b481523",
   "metadata": {},
   "source": [
    "### `labs.csv`\n",
    "\n",
    "This file contains dynamic data quantifying fictional subject laboratory test measurements. Each row of this file contains a record of a particular lab test measured for a subject. Note that the lab data is not organized into separate columns for each lab; rather each row contains a pair of a lab test name and the associated value; this is what we call in ESGPT a \"multivariate regression\" column encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d05a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       MRN | timestamp           | lab_name  | lab_value |\r\n",
      "| --------- | ------------------- | --------- | --------- |\r\n",
      "| 1,063,040 | 15:54:09-2010-09-02 | SpO2      |     50.00 |\r\n",
      "|   142,258 | 13:07:35-2010-08-21 | SpO2      |     50.00 |\r\n",
      "|   659,318 | 13:50:19-2010-11-10 | GCS       |     15.00 |\r\n",
      "|   113,556 | 09:45:04-2010-04-28 | potassium |      5.26 |\r\n",
      "|   671,425 | 02:47:40-2010-04-02 | SpO2      |     50.00 |\r\n",
      "|       ... | ...                 | ...       |       ... |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" --max-rows=5 raw/labs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcd527",
   "metadata": {},
   "source": [
    "# Processing Synthetic Data with ESGPT\n",
    "\n",
    "Now that we see the form of this synthetic data, we can examine how to process it with Event Stream GPT. From\n",
    "the base directory of the ESGPT repository, we can run the following command:\n",
    "\n",
    "```bash\n",
    "PYTHONPATH=$(pwd):$PYTHONPATH ./scripts/build_dataset.py \\\n",
    "\t--config-path=\"$(pwd)/sample_data/\" \\\n",
    "\t--config-name=dataset \\\n",
    "\t\"hydra.searchpath=[$(pwd)/configs]\"\n",
    "```\n",
    "\n",
    "```{note}\n",
    "This script, like all built-in ESGPT scripts, uses [Hydra](https://hydra.cc/), a configuration file and experiment run-script library. In hydra, all scripts can take as input a set of composable configuration files which can be overwritten via files or viat he command line. If you aren't already familiar with Hydra, you should read through some of their examples or tutorials to gain some familiarity with their system.\n",
    "```\n",
    "\n",
    "Before we actually run this command, we need to do 2 things:\n",
    "  1. Decide what we _want_ the command to do, conceptually.\n",
    "  2. Understand what we're _telling_ the library to do, via its input arguments.\n",
    "  \n",
    "## What do we _want_ to happen?\n",
    "We can see that our synthetic data has a few different kinds of things happening to these subjects. In the ESGPT data model, we want to organize this data so that we clearly know who our subjects are, quantify when things happen to those subjects, and record in a sparse manner what is happening to those patients. Let's list a few more specific desiderata:\n",
    "  1. We should expect our system to quantify those subjects in our synthetic data that meet our inclusion criteria (which we haven't yet specified).\n",
    "  2. The system should bucket all interactions for subjects into appropriately defined events, across admissions, discharges, vitals signs, and laboratory tests.\n",
    "  3. The system should learn appropriate categorical vocabularies, numerical outlier detector models, numerical normalization models, for the various measurements we want to extract (which we haven't yet specified).\n",
    "  4. The system should produce \"deep-learning friendly\" representations of these data.\n",
    "\n",
    "```{note}\n",
    "A quick tangent -- what do we mean by \"deep-learning friendly\" representations of these data? Well, right now, if we were to try to run these data through _any_ deep-learning system for longitudinal data, we'd need to re-format these data such that it is easy to efficiently (ideally $O(1)$) retrieve all data corresponding to a single subject in an organized timeseries format that we can then efficiently (meaning in a manner requiring minimal GPU memory) pass into a sequential neural network. \n",
    "\n",
    "In the current representation, this retrieval process would not be $O(1)$; instead, if we didn't modify the data's organization at all, for each new MRN, we'd need to select from each data file all those rows with that MRN (each selection being an $O(N)$ operation), and then we would need to subsequently sort all the temporal data by timestamp (another $O(L\\ln(L))$ operation).\n",
    "\n",
    "Similarly, if we use a naive, dense encoding of the data per measurement for our DL representation, this will be very wasteful in terms of GPU memory, as each record will need to occupy memory proportionate to the total number of possible measurements we could observe in our data (e.g., the total number of lab tests, plus the total number of vitals signs, plus the total number of admission departments, etc.). Instead, a sparse encoding should be used.\n",
    "\n",
    "These two properties are exactly what we mean by a \"deep-learning friendly\" representation of the data.\n",
    "```\n",
    "\n",
    "We can see that there are several questions posed by these desiderata that we need to answer, such as:\n",
    "  1. What are our inclusion criteria?\n",
    "  2. How should we bucket interactions into events?\n",
    "  3. What measurements do we want to extract?\n",
    "  4. How do we want to define \"outliers\"?\n",
    "  5. How do we define \"appropriate categorical vocabularies\"?\n",
    "  6. How do we want to normalize numerical measurements?\n",
    "  \n",
    "To start us off, let's use the following answers:\n",
    "  1. We'll include all subjects who have at least 3 events, with no other inclusion/exclusion criteria.\n",
    "  2. We'll define an \"event\" to be any interactions happening to a patient within a 1 hour period. We'll bucket these interactions together starting at the earliest event.\n",
    "  3. Ideally, we'd like to extract _all_ measurements. As we'll see, however, due to a limitation in the current version of ESGPT, we'll extract all measurements except for the patient's height. In particular, we'll extract the occurrence of admissions, discharges, vitals signs, and laboratory tests, as well as the subject's age, eye color,  admission department, the values recorded for HR and temperature, and all lab test values.\n",
    "  4. We'll use a very simple outlier model, that excludes numerical data as outliers if their values exceed 1.5 standard deviations from the mean. This is an extremely aggressive cutoff only suitable for this synthetic data setting.\n",
    "  5. We'll keep any categorical observation as a vocabulary element if it occurs at least 5 times.\n",
    "  6. We'll normalize our numerical observations to have zero mean and unit variance.\n",
    "  \n",
    "## Telling the pipeline what to do: input config\n",
    "Now that we have some basic idea of what we want the pipeline to do, let's examine the input configuration file that we pass to the dataset script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c11c506",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\r\n",
      "  - dataset_base\r\n",
      "  - _self_\r\n",
      "\r\n",
      "# So that it can be run multiple times without issue.\r\n",
      "do_overwrite: True\r\n",
      "\r\n",
      "cohort_name: \"sample\"\r\n",
      "subject_id_col: \"MRN\"\r\n",
      "raw_data_dir: \"./sample_data/raw/\"\r\n",
      "save_dir: \"./sample_data/processed/${cohort_name}\"\r\n",
      "\r\n",
      "DL_chunk_size: null\r\n",
      "\r\n",
      "inputs:\r\n",
      "  subjects:\r\n",
      "    input_df: \"${raw_data_dir}/subjects.csv\"\r\n",
      "  admissions:\r\n",
      "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\r\n",
      "    start_ts_col: \"admit_date\"\r\n",
      "    end_ts_col: \"disch_date\"\r\n",
      "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\r\n",
      "    event_type: [\"OUTPATIENT_VISIT\", \"ADMISSION\", \"DISCHARGE\"]\r\n",
      "  vitals:\r\n",
      "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\r\n",
      "    ts_col: \"vitals_date\"\r\n",
      "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\r\n",
      "  labs:\r\n",
      "    input_df: \"${raw_data_dir}/labs.csv\"\r\n",
      "    ts_col: \"timestamp\"\r\n",
      "    ts_format: \"%H:%M:%S-%Y-%m-%d\"\r\n",
      "\r\n",
      "measurements:\r\n",
      "  static:\r\n",
      "    single_label_classification:\r\n",
      "      subjects: [\"eye_color\"]\r\n",
      "  functional_time_dependent:\r\n",
      "    age:\r\n",
      "      functor: AgeFunctor\r\n",
      "      necessary_static_measurements: { \"dob\": [\"timestamp\", \"%m/%d/%Y\"] }\r\n",
      "      kwargs: { dob_col: \"dob\" }\r\n",
      "  dynamic:\r\n",
      "    multi_label_classification:\r\n",
      "      admissions: [\"department\"]\r\n",
      "    univariate_regression:\r\n",
      "      vitals: [\"HR\", \"temp\"]\r\n",
      "    multivariate_regression:\r\n",
      "      labs: [[\"lab_name\", \"lab_value\"]]\r\n",
      "\r\n",
      "outlier_detector_config:\r\n",
      "  stddev_cutoff: 1.5\r\n",
      "min_valid_vocab_element_observations: 5\r\n",
      "min_valid_column_observations: 5\r\n",
      "min_true_float_frequency: 0.1\r\n",
      "min_unique_numerical_observations: 20\r\n",
      "min_events_per_subject: 3\r\n",
      "agg_by_time_scale: \"1h\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat dataset.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ddfa1",
   "metadata": {},
   "source": [
    "There are a number of sections in this file. Firstly, the first three lines ensure this config builds on the defaults provided with the ESGPT library, via Hydra's normal mechanisms. If you aren't familiar with this syntax, check out the [Hydra documentation](https://hydra.cc/docs/1.3/advanced/defaults_list/).\n",
    "\n",
    "Next, there is a section defining some overarching variables and a section defining our input sources. We can see this section details the paths to each of our input files as well as the formatting used for (most of) the timestamps within these files. Note that this section makes use of [Hydra/OmegaConf's Interpolations](https://omegaconf.readthedocs.io/en/2.3_branch/grammar.html#interpolation-strings) to simplify the specification of the file paths used. \n",
    "\n",
    "```{warning}\n",
    "Two parameters in this section are required: `subject_id_col`, and `cohort_name`. This will be explored in more detail later in this tutorial.\n",
    "```\n",
    "\n",
    "Next, we have a section defining the various measurements we'll exctract in this dataset. We can see we specify each of the measurements we discussed above:\n",
    "  1. `eye_color` is extracted as a `static`, `single_label_classification` measure. \n",
    "  2. `age` is extracted as a `functional_time_dependent` measure, leveraging the date-of-birth column `dob`. _Note that this is where we define the timestamp format for the `dob` column, as it is a timestamp formatted static column!_\n",
    "  3. `department` is extracted as a `dynamic`, `multi_label_classification` measure.\n",
    "  4. `HR`, and `temp` are extracted as `dynamic`, `univariate_regression` measures.\n",
    "  5. `lab_name` and `lab_value` are extracted as a single `dynamic`, `multivariate_regression` measure.\n",
    "  \n",
    "```{note}\n",
    "The terms `static`, `functional_time_dependent`, & `dynamic` and `single_label_classification`, `multi_label_classification`, `univariate_regression`, and `multivariate_regression`, are defined enumerations in the `EventStream.data.config` sub-module, and dictate where measurements are stored and how they are pre-processed.\n",
    "```\n",
    "  \n",
    "Finally, we have the remaining set of parameters, which define our inclusion-exclusion criteria (by specifying `min_events_per_subject`), our outlier and normalizer model configuration parameters (`normalization` being omitted here as what we want is the default value), our filtering thresholds for vocabulary elements, and the aggregation time-scale for events.\n",
    "\n",
    "### What else _could_ we have specified?\n",
    "To better understand the structure of this input specification, let's explore this input configuration file in a bit more detail. To start with, let's look at what the default, base config contains (the config we inherit from in the defaults list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34bed35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\r\n",
      "  - outlier_detector_config: stddev_cutoff\r\n",
      "  - normalizer_config: standard_scaler\r\n",
      "  - _self_\r\n",
      "\r\n",
      "cohort_name: ???\r\n",
      "save_dir: ${oc.env:PROJECT_DIR}/data/${cohort_name}\r\n",
      "subject_id_col: ???\r\n",
      "seed: 1\r\n",
      "split: [0.8, 0.1]\r\n",
      "do_overwrite: False\r\n",
      "DL_chunk_size: 20000\r\n",
      "min_valid_vocab_element_observations: 25\r\n",
      "min_valid_column_observations: 50\r\n",
      "min_true_float_frequency: 0.1\r\n",
      "min_unique_numerical_observations: 25\r\n",
      "min_events_per_subject: 20\r\n",
      "agg_by_time_scale: null\r\n",
      "\r\n",
      "hydra:\r\n",
      "  job:\r\n",
      "    name: build_${cohort_name}\r\n",
      "  run:\r\n",
      "    dir: ${save_dir}/.logs\r\n",
      "  sweep:\r\n",
      "    dir: ${save_dir}/.logs\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../configs/dataset_base.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eb1cd",
   "metadata": {},
   "source": [
    "We can see there are some parameters we're familiar with and some we're not. Firstly, we can see that this default base config marks `cohort_name` and `subject_id_col` with `???`. This is the OmegaConf provided value to represent a value that _needs to be overwritten_ in downstream usage. This is why those two parameters are mandatory. This config also has variables for the seed, split size, and some hydra-internal parameters. Further, it points to two  further default configs for the outlier detector and normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1b5f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls: stddev_cutoff\r\n",
      "stddev_cutoff: 5.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../configs/outlier_detector_config/stddev_cutoff.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "723c10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls: standard_scaler\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../configs/normalizer_config/standard_scaler.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0888ce",
   "metadata": {},
   "source": [
    "These are both quite simple, but show how the final config will be constructed from these values. \n",
    "\n",
    "One thing that is notably missing from this broader structure is any notion of included `inputs` or `measurements` sections. To understand how we can further specify our config, we need to understand how we could modify those sections as well.\n",
    "\n",
    "#### Inputs\n",
    "This section allows us to specify which input data frames should be read, and from where. The `inputs:` option should be an object whose keys are the names of input sources and whose values are configuration for those inputs. Currently, two input formats are possible:\n",
    "  1. The `input_df` format, which is used in this synthetic example. This format has an input configuration that contains the `input_df:` key whose value is a file path pointing to a `csv` or `parquet` data-frame file on disk that contains that input source's data. For example:\n",
    "  ```yaml\n",
    "admissions:\n",
    "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\n",
    "    start_ts_col: \"admit_date\"\n",
    "    end_ts_col: \"disch_date\"\n",
    "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\n",
    "    event_type: [\"OUTPATIENT_VISIT\", \"ADMISSION\", \"DISCHARGE\"]\n",
    "  ```\n",
    "  2. The `query` format, which is used in the MIMIC-IV tutorial. In this format, you must specify a `query` parameter. This parameter can either be a string query or a list of string queries, in which case you must specify a global `connection_uri` parameter detailing the URI of the database to which you wish to query (In the [connector-x](https://github.com/sfu-db/connector-x) format), or a dictionary, with keys and values specifying parameters of the [`EventStream.data.dataset_polars.Query`]() object. For example:\n",
    "  ```yaml\n",
    "patients:\n",
    "    query: |-\n",
    "      SELECT subject_id, gender, to_date((anchor_year-anchor_age)::CHAR(4), 'YYYY') AS year_of_birth\n",
    "      FROM mimiciv_hosp.patients\n",
    "      WHERE subject_id IN (\n",
    "        SELECT long_icu.subject_id FROM (\n",
    "          (\n",
    "            SELECT subject_id FROM mimiciv_icu.icustays WHERE los > ${min_los}\n",
    "          ) AS long_icu INNER JOIN (\n",
    "            SELECT subject_id\n",
    "            FROM mimiciv_hosp.admissions\n",
    "            GROUP BY subject_id\n",
    "            HAVING COUNT(*) > ${min_admissions}\n",
    "          ) AS many_admissions\n",
    "          ON long_icu.subject_id = many_admissions.subject_id\n",
    "        )\n",
    "      )\n",
    "    must_have: [\"gender\", \"year_of_birth\"]\n",
    "  ```\n",
    "  \n",
    "Each input can also have a number of other keys and values, including:\n",
    "\n",
    "##### Timestamp & Event-type Specification.\n",
    "For non-static data sources, the keys `ts_col` or `start_ts_col` and `end_ts_col` specify the name of the column (or columns) containing the timestamp for the event, and `ts_format` the format of that timestamp. `ts_col` is used for data-sources where each row represents one event, and `start_`/`end_ts_col` for data-sources where each row specifies a range in time. For example, in our synthetic config,\n",
    "```yaml\n",
    "admissions:\n",
    "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\n",
    "    start_ts_col: \"admit_date\"\n",
    "    end_ts_col: \"disch_date\"\n",
    "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\n",
    "    event_type: [\"OUTPATIENT_VISIT\", \"ADMISSION\", \"DISCHARGE\"]\n",
    "```\n",
    "\n",
    "specifies a range event, where the start timestamp is stored in `admit_date` and the end timestamp in `disch_date`, formatted as `\"%m/%d/%Y, %H:%M:%S\"`. In contrast,\n",
    "\n",
    "```yaml\n",
    "labs:\n",
    "    input_df: \"${raw_data_dir}/labs.csv\"\n",
    "    ts_col: \"timestamp\"\n",
    "    ts_format: \"%H:%M:%S-%Y-%m-%d\"\n",
    "```\n",
    "\n",
    "captures data where each row is a single-timepoint event, with timestamp stored in `\"%H:%M:%S-%Y-%m-%d\"` format in column `timestamp`.\n",
    "\n",
    "You can also explicitly set the type of each event. Events' types in ESGPT are categorical variables defined by the user that are used to dictate any intra-event causal dependency graphs in downstream models, can be used to help define downstream tasks, and are otherwise used to analyze and describe data. When using the pre-defined build dataset script, they can either be explicitly set or are automatically inferred from the name of the input block. For example, in the examples given above, the `labs:` block produces an input source with the event type `LAB` (the singular, upper-cased inflection of the name of the block, `'labs'`), and `admissions` (being a range event) produces events of type `'OUTPATIENT_VISIT'` when `admit_date == disch_date` and `'ADMISSION'` on `admit_date` and `'DISCHARGE'` on `disch_date`.\n",
    "```{note}\n",
    "For range events, the default event types are defined to be `*_EQ`, `*_START`, and `*_END`, where `*` is the singular, upper-cased inflection of the input block name.\n",
    "```\n",
    "\n",
    "Event types can also be defined to be column dependent. For example, in this config example (which is not part of our current synthetic example config), we see that event types are defined to take on the value of the column `'visit_occurrence_concept_name'` for the case that the start and end times are the same and for start events, but the static `'Drug Stop'` for end events.\n",
    "\n",
    "```yaml\n",
    "drugs:\n",
    "    input_df: \"${raw_data_dir}/drug.parquet\"\n",
    "    start_ts_col: \"drug_exposure_start_datetime\"\n",
    "    end_ts_col: [\"drug_exposure_end_datetime\", \"verbatim_end_date\"]\n",
    "    event_type: [\"COL:visit_occurrence_concept_name\", \"COL:visit_occurrence_concept_name\", \"Drug Stop\"]\n",
    "    start_columns: {\"standard_concept_name\": \"drug\", \"drug_type_concept_name\": \"drug_type\"}\n",
    "    end_columns: \n",
    "        standard_concept_name: drug\n",
    "        drug_type_concept_name: drug_type\n",
    "        stop_reason: drug_stop_reason\n",
    "```\n",
    "\n",
    "##### Filtering\n",
    "You can also specify a simple filter used for a given input source. For example, in the `patients` block in the MIMIC-IV example, we specify that valid rows must have `'gender'` and `'year_of_birth'` defined and non-null. This is another way to enforce cohort inclusion/exclusion criteria. The filter object can either be a list of strings, in which case those columns must have non-null values, or a dictionary from column names to either the boolean `True` (indicating the column must be present and non-null) or lists of allowable values for that column.\n",
    "\n",
    "##### Measurement columns to extract\n",
    "You can also specify which measurements should be extracted to associate with a given input data source. Largely, this information will be determined automatically based on the `measurements` section of the config; however, it can be specified explicitly as well. The most common case this would be done is to differentiate different measurements to associate with `start` and `end` events for range events or to re-name measurements from their input column names to new names for internal use (this can be done not only for cosmetic reasons, but so as to unify or disentangle measurements across different input files). For example, in the `drugs:` example shown above, the columns `standard_concept_name` and `drug_type_concept_name` are both used for both start and end events, and are renamed to `'drug'`, and `'drug_type'` in both cases, whereas `stop_reason` is used only for end events (and is renamed to `drug_stop_reason`).\n",
    "#### Measurements Section\n",
    "The `measurements:` block lists all the actual measurements that should be extracted from those input sources, broken down into categories based on their `temporality` and `modality` (see `EventStream.data.types.TemporalityType` and `EventStream.data.types.DataModality`, respectively). \n",
    "\n",
    "The only non-standard portion of this block corresponds to the `functional_time_dependent` block, which specifies measurements whose values are _not_ stored in the raw input data by default, but are instead computable dynamically given per-subject static data and the timestamps of other events that occur in the data. A good example is a subject's age, which is included in our synthetic configuration. Given a subject's date-of-birth and the timestamp of any other event, we can dynamically compute the subject's age as of that event, which is exactly what the `EventStream.data.time_dependent_functor.AgeFunctor` does.\n",
    "\n",
    "The structure of this config section is\n",
    "```yaml\n",
    "functional_time_dependent:\n",
    "  output_measurement_name:\n",
    "      functor: ??? # The functor that is used for this measurement. Must be in `EventStream.data.config.MeasurementConfig.FUNCTORS`\n",
    "      necessary_static_measurements: { \"static_measurement_column\": ??? } # column name: column formatting info\n",
    "      kwargs: { kwarg: kwval } # Keyword args to pass to functor constructor.\n",
    "```\n",
    "\n",
    "Currently, only [`AgeFunctor`]() and [`TimeOfDayFunctor`] are pre-defined and supported, but this can be extended by the user by directly adding new functors to the [`EventStream.data.config.MeasurementConfig`]() object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d861ab8",
   "metadata": {},
   "source": [
    "## Running the Command\n",
    "Now that we understand the setup a bit better, let's run the actual command:\n",
    "\n",
    "```bash\n",
    "PYTHONPATH=$(pwd):$PYTHONPATH ./scripts/build_dataset.py \\\n",
    "\t--config-path=\"$(pwd)/sample_data/\" \\\n",
    "\t--config-name=dataset \\\n",
    "\t\"hydra.searchpath=[$(pwd)/configs]\"\n",
    "```\n",
    "\n",
    "You should see as output the printed line `Empty new events dataframe of type OUTPATIENT_VISIT!`, but\n",
    "otherwise nothing. Before we proceed further, let's break down what this process has done, and how it could do\n",
    "things differently. \n",
    "\n",
    "### What happened, qualitatively?\n",
    "\n",
    "When we run this script, what happens, at a high level, can be broken down into the following steps:\n",
    "\n",
    "#### Step 1: Config Parsing\n",
    "First, the script parses our input config file into a slightly refined structured form, then passes that as input to the `EventStream.data.dataset_polars.Dataset` constructor.\n",
    "\n",
    "To see what this process looks like, we can inspect one portion of the output of the overall script, which we can find in the [sample_data/processed/sample]() directory; in particular, the `input_schema.json` file.\n",
    "```{note}\n",
    "The `sample_data/processed/sample` directory is the `save_dir` key in our `dataset.yaml` configuration file.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6db36770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"static\": {\r\n",
      "        \"input_df\": \"./sample_data/raw//subjects.csv\",\r\n",
      "        \"type\": \"static\",\r\n",
      "        \"event_type\": null,\r\n",
      "        \"subject_id_col\": \"MRN\",\r\n",
      "        \"ts_col\": null,\r\n",
      "        \"start_ts_col\": null,\r\n",
      "        \"end_ts_col\": null,\r\n",
      "        \"ts_format\": null,\r\n",
      "        \"start_ts_format\": null,\r\n",
      "        \"end_ts_format\": null,\r\n",
      "        \"data_schema\": [\r\n",
      "            {\r\n",
      "                \"eye_color\": \"categorical\",\r\n",
      "                \"dob\": [\r\n",
      "                    \"dob\",\r\n",
      "                    [\r\n",
      "                        \"timestamp\",\r\n",
      "                        \"%m/%d/%Y\"\r\n",
      "                    ]\r\n",
      "                ]\r\n",
      "            }\r\n",
      "        ],\r\n",
      "        \"start_data_schema\": null,\r\n",
      "        \"end_data_schema\": null,\r\n",
      "        \"must_have\": []\r\n",
      "    },\r\n",
      "    \"dynamic\": [\r\n",
      "        {\r\n",
      "            \"input_df\": \"./sample_data/raw//admit_vitals.csv\",\r\n",
      "            \"type\": \"range\",\r\n",
      "            \"event_type\": [\r\n",
      "                \"OUTPATIENT_VISIT\",\r\n",
      "                \"ADMISSION\",\r\n",
      "                \"DISCHARGE\"\r\n",
      "            ],\r\n",
      "            \"subject_id_col\": \"MRN\",\r\n",
      "            \"ts_col\": null,\r\n",
      "            \"start_ts_col\": \"admit_date\",\r\n",
      "            \"end_ts_col\": \"disch_date\",\r\n",
      "            \"ts_format\": null,\r\n",
      "            \"start_ts_format\": \"%m/%d/%Y, %H:%M:%S\",\r\n",
      "            \"end_ts_format\": \"%m/%d/%Y, %H:%M:%S\",\r\n",
      "            \"data_schema\": [\r\n",
      "                {\r\n",
      "                    \"department\": \"categorical\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"start_data_schema\": [\r\n",
      "                {\r\n",
      "                    \"department\": \"categorical\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"end_data_schema\": [\r\n",
      "                {\r\n",
      "                    \"department\": \"categorical\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"must_have\": []\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"input_df\": \"./sample_data/raw//admit_vitals.csv\",\r\n",
      "            \"type\": \"event\",\r\n",
      "            \"event_type\": \"VITAL\",\r\n",
      "            \"subject_id_col\": \"MRN\",\r\n",
      "            \"ts_col\": \"vitals_date\",\r\n",
      "            \"start_ts_col\": null,\r\n",
      "            \"end_ts_col\": null,\r\n",
      "            \"ts_format\": \"%m/%d/%Y, %H:%M:%S\",\r\n",
      "            \"start_ts_format\": null,\r\n",
      "            \"end_ts_format\": null,\r\n",
      "            \"data_schema\": [\r\n",
      "                {\r\n",
      "                    \"HR\": \"float\",\r\n",
      "                    \"temp\": \"float\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"start_data_schema\": null,\r\n",
      "            \"end_data_schema\": null,\r\n",
      "            \"must_have\": []\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"input_df\": \"./sample_data/raw//labs.csv\",\r\n",
      "            \"type\": \"event\",\r\n",
      "            \"event_type\": \"LAB\",\r\n",
      "            \"subject_id_col\": \"MRN\",\r\n",
      "            \"ts_col\": \"timestamp\",\r\n",
      "            \"start_ts_col\": null,\r\n",
      "            \"end_ts_col\": null,\r\n",
      "            \"ts_format\": \"%H:%M:%S-%Y-%m-%d\",\r\n",
      "            \"start_ts_format\": null,\r\n",
      "            \"end_ts_format\": null,\r\n",
      "            \"data_schema\": [\r\n",
      "                {\r\n",
      "                    \"lab_name\": \"categorical\",\r\n",
      "                    \"lab_value\": \"float\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"start_data_schema\": null,\r\n",
      "            \"end_data_schema\": null,\r\n",
      "            \"must_have\": []\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat processed/sample/input_schema.json | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ddf01",
   "metadata": {},
   "source": [
    "This object, stored in JSON format, is an instance of the `EventStream.data.config.DatasetSchema` object; interested readers can read more about it's specific formatting requirements there.\n",
    "\n",
    "#### Step 2: Data reading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbdd6b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subject_id</th><th>MRN</th><th>eye_color</th><th>dob</th></tr><tr><td>u8</td><td>cat</td><td>cat</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>0</td><td>&quot;310243&quot;</td><td>&quot;GREEN&quot;</td><td>1981-07-28 00:00:00</td></tr><tr><td>1</td><td>&quot;384198&quot;</td><td>&quot;BROWN&quot;</td><td>1985-04-15 00:00:00</td></tr><tr><td>2</td><td>&quot;520533&quot;</td><td>&quot;BROWN&quot;</td><td>1979-04-15 00:00:00</td></tr><tr><td>3</td><td>&quot;850710&quot;</td><td>&quot;HAZEL&quot;</td><td>1970-08-08 00:00:00</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 4)\n",
       "┌────────────┬────────┬───────────┬─────────────────────┐\n",
       "│ subject_id ┆ MRN    ┆ eye_color ┆ dob                 │\n",
       "│ ---        ┆ ---    ┆ ---       ┆ ---                 │\n",
       "│ u8         ┆ cat    ┆ cat       ┆ datetime[μs]        │\n",
       "╞════════════╪════════╪═══════════╪═════════════════════╡\n",
       "│ 0          ┆ 310243 ┆ GREEN     ┆ 1981-07-28 00:00:00 │\n",
       "│ 1          ┆ 384198 ┆ BROWN     ┆ 1985-04-15 00:00:00 │\n",
       "│ 2          ┆ 520533 ┆ BROWN     ┆ 1979-04-15 00:00:00 │\n",
       "│ 3          ┆ 850710 ┆ HAZEL     ┆ 1970-08-08 00:00:00 │\n",
       "└────────────┴────────┴───────────┴─────────────────────┘"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.scan_parquet('processed/sample/subjects_df.parquet').head(4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9abe57f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>subject_id</th><th>timestamp</th><th>event_type</th><th>age</th><th>age_is_inlier</th></tr><tr><td>u32</td><td>u8</td><td>datetime[μs]</td><td>cat</td><td>f64</td><td>bool</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>2010-06-24 13:23:00</td><td>&quot;ADMISSION&amp;VITA…</td><td>-0.540896</td><td>true</td></tr><tr><td>1</td><td>0</td><td>2010-06-24 14:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.540871</td><td>true</td></tr><tr><td>2</td><td>0</td><td>2010-06-24 15:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.540845</td><td>true</td></tr><tr><td>3</td><td>0</td><td>2010-06-24 16:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.54082</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 6)\n",
       "┌──────────┬────────────┬─────────────────────┬─────────────────────┬───────────┬───────────────┐\n",
       "│ event_id ┆ subject_id ┆ timestamp           ┆ event_type          ┆ age       ┆ age_is_inlier │\n",
       "│ ---      ┆ ---        ┆ ---                 ┆ ---                 ┆ ---       ┆ ---           │\n",
       "│ u32      ┆ u8         ┆ datetime[μs]        ┆ cat                 ┆ f64       ┆ bool          │\n",
       "╞══════════╪════════════╪═════════════════════╪═════════════════════╪═══════════╪═══════════════╡\n",
       "│ 0        ┆ 0          ┆ 2010-06-24 13:23:00 ┆ ADMISSION&VITAL&LAB ┆ -0.540896 ┆ true          │\n",
       "│ 1        ┆ 0          ┆ 2010-06-24 14:23:00 ┆ VITAL&LAB           ┆ -0.540871 ┆ true          │\n",
       "│ 2        ┆ 0          ┆ 2010-06-24 15:23:00 ┆ VITAL&LAB           ┆ -0.540845 ┆ true          │\n",
       "│ 3        ┆ 0          ┆ 2010-06-24 16:23:00 ┆ VITAL&LAB           ┆ -0.54082  ┆ true          │\n",
       "└──────────┴────────────┴─────────────────────┴─────────────────────┴───────────┴───────────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.scan_parquet('processed/sample/events_df.parquet').head(4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2afdd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>measurement_id</th><th>department</th><th>HR</th><th>&hellip;</th><th>HR_is_inlier</th><th>temp_is_inlier</th><th>lab_name_is_inlier</th></tr><tr><td>u32</td><td>cat</td><td>f64</td><td>&hellip;</td><td>bool</td><td>bool</td><td>bool</td></tr></thead><tbody><tr><td>0</td><td>&quot;CARDIAC&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 10)\n",
       "┌────────────────┬────────────┬──────┬──────┬───┬──────────────┬────────────────┬──────────────────┐\n",
       "│ measurement_id ┆ department ┆ HR   ┆ temp ┆ … ┆ HR_is_inlier ┆ temp_is_inlier ┆ lab_name_is_inli │\n",
       "│ ---            ┆ ---        ┆ ---  ┆ ---  ┆   ┆ ---          ┆ ---            ┆ er               │\n",
       "│ u32            ┆ cat        ┆ f64  ┆ f64  ┆   ┆ bool         ┆ bool           ┆ ---              │\n",
       "│                ┆            ┆      ┆      ┆   ┆              ┆                ┆ bool             │\n",
       "╞════════════════╪════════════╪══════╪══════╪═══╪══════════════╪════════════════╪══════════════════╡\n",
       "│ 0              ┆ CARDIAC    ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "│ 1              ┆ PULMONARY  ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "│ 2              ┆ PULMONARY  ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "│ 3              ┆ PULMONARY  ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "└────────────────┴────────────┴──────┴──────┴───┴──────────────┴────────────────┴──────────────────┘"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.scan_parquet('processed/sample/dynamic_measurements_df.parquet').head(4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a868ab3",
   "metadata": {},
   "source": [
    "### Inspecting the _actual_ output\n",
    "\n",
    "The entire output dataset is stored in the [sample_data/processed/sample](<>) directory. Let's inspect its\n",
    "contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "198d4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18M\r\n",
      "drwxrwxr-x 5 mmd mmd 4.0K Jul 14 17:14 \u001b[0m\u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.8K Jul 14 17:16 config.json\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34mDL_reps\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd  11M Jul 14 17:14 dynamic_measurements_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 5.2K Jul 14 17:14 E.pkl\r\n",
      "-rw-rw-r-- 1 mmd mmd 6.9M Jul 14 17:14 events_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.5K Jul 14 17:16 hydra_config.yaml\r\n",
      "-rw-rw-r-- 1 mmd mmd 3.0K Jul 14 17:16 inferred_measurement_configs.json\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34minferred_measurement_metadata\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.7K Jul 14 17:16 input_schema.json\r\n",
      "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m.logs\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 2.7K Jul 14 17:14 subjects_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd  773 Jul 14 17:16 vocabulary_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls --color -lah processed/sample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd76eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29M\tprocessed/sample/\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh processed/sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac71cca",
   "metadata": {},
   "source": [
    "We can see that this directory contains a set of files and sub-directories, and that in total it takes up only\n",
    "30 MB of disk space. Note that this is in contrast to the original, raw data, which took 53 MB on disk.\n",
    "\n",
    "Each of these files contains different information about this synthetic dataset. Let's inspect them and see\n",
    "what they contain. We'll go in a rough order that corresponds with where these files fit into the broader\n",
    "pipeline.\n",
    "\n",
    "#### Input & Logging Files\n",
    "\n",
    "##### `hydra_config.yaml`\n",
    "\n",
    "This file contains the full, resolved hydra input config to the dataset script. Whereas [`dataset.yaml`](<>)\n",
    "(the input config file used in the script run above) relies on some default values in the built-in\n",
    "[`dataset_base.yaml`](<>) config, the `hydra_config.yaml` is fully self-sufficient and can be used to reproduce\n",
    "the pipeline run in its entirety. In this case, it is very similar to the [`dataset.yaml`](<>) file which we'll\n",
    "inspect in more detail later, so we won't include it here.\n",
    "\n",
    "##### `input_schema.json`\n",
    "\n",
    "This file contains a processed version of the input data frame schemas used to read the raw data from disk. It\n",
    "is produced from the input [`dataset.yaml`](<>) config file (which we'll discuss in more detail later), and is\n",
    "stored in JSON format. This can be helpful to validate exactly what sources were read in what way. It is not\n",
    "used in any downstream pipeline components, so is not essential to understand.\n",
    "\n",
    "```{literalinclude} ../../sample_data/processed/sample/input_schema.json\n",
    "---\n",
    "language: json\n",
    "---\n",
    "```\n",
    "\n",
    "##### `.logs`\n",
    "\n",
    "This sub-directory contains a hydra run log file for this dataset build run. As the pipeline currently doesn't\n",
    "take advantage of the python logging module at all, the files it contains are empty.\n",
    "\n",
    "```bash\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example(+6/-5)+* ± ls .logs/build_sample.log\n",
    ".logs/build_sample.log\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example(+6/-5)+* ± cat .logs/build_sample.log\n",
    "```\n",
    "\n",
    "#### Output Files\n",
    "\n",
    "##### Dataset Configuration & Learned Measurement Metadata\n",
    "\n",
    "###### `config.json`\n",
    "\n",
    "This is the dataset's input config file. It contains the input measurement specifications and control\n",
    "parameters for the dataset pipeline. This is largely set from the input `dataset.yaml` config.\n",
    "\n",
    "###### `inferred_measurement_configs.json` & `inferred_measurement_metadata`\n",
    "\n",
    "These represent the inferred pre-processing parameters for the inferred measurements. This is stored in two\n",
    "forms: First, most data about the inferred measurements is stored in a flat JSON file,\n",
    "`inferred_measurement_configs.json`. This file contains an object whose keys are measurement names and whose\n",
    "values are configuration objects describing the measurements.\n",
    "\n",
    "The full file looks like this:\n",
    "\n",
    "```{literalinclude} ../../sample_data/processed/sample/inferred_measurement_configs.json\n",
    "---\n",
    "language: json\n",
    "---\n",
    "```\n",
    "\n",
    "To isolate a single measurement, we can examine the configuration for `'eye_color'`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"eye_color\": {\n",
    "    \"name\": \"eye_color\",\n",
    "    \"temporality\": \"static\",\n",
    "    \"modality\": \"single_label_classification\",\n",
    "    \"observation_frequency\": 1.0,\n",
    "    \"functor\": null,\n",
    "    \"vocabulary\": {\n",
    "      \"vocabulary\": [\"UNK\", \"BROWN\", \"BLUE\", \"HAZEL\", \"GREEN\"],\n",
    "      \"obs_frequencies\": [0.0, 0.5125, 0.2125, 0.175, 0.1]\n",
    "    },\n",
    "    \"values_column\": null,\n",
    "    \"_measurement_metadata\": null\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We can see that this configuration object details several facts about the eye color measurement:\n",
    "\n",
    "- That it is a static, single-label classification measurement (these were specified in the input config)\n",
    "- That this is observed on 100% of subjects.\n",
    "- That the relative frequencies of the categories \"Brown\", \"Blue\", \"Hazel\", \"Green\" are 51.25%, 21.25%,\n",
    "  17.5%, and 10%, respectively.\n",
    "\n",
    "To see a different measurement, one that is a multivariate regression measurement, we can inspect the lab\n",
    "tests measurement configs:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"lab_name\": {\n",
    "    \"name\": \"lab_name\",\n",
    "    \"temporality\": \"dynamic\",\n",
    "    \"modality\": \"multivariate_regression\",\n",
    "    \"observation_frequency\": 0.9953452513588434,\n",
    "    \"functor\": null,\n",
    "    \"vocabulary\": {\n",
    "      \"vocabulary\": [\n",
    "        \"UNK\", \"SpO2\", \"creatinine\", \"potassium\", \"SOFA__EQ_1\", \"GCS__EQ_1\", \"SOFA__EQ_2\",\n",
    "        \"SOFA__EQ_3\", \"GCS__EQ_4\", \"GCS__EQ_3\", \"GCS__EQ_2\", \"GCS__EQ_5\", \"GCS__EQ_6\", \"GCS__EQ_7\",\n",
    "        \"SOFA__EQ_4\", \"GCS__EQ_8\", \"GCS__EQ_9\", \"GCS__EQ_10\", \"GCS__EQ_11\", \"GCS__EQ_15\", \"GCS__EQ_12\",\n",
    "        \"GCS__EQ_13\", \"GCS__EQ_14\", \"SOFA__EQ_1000000\", \"GCS__EQ_1000000\"\n",
    "      ],\n",
    "      \"obs_frequencies\": [\n",
    "        0.0, 0.8259984895186395, 0.04326148962598335, 0.042245556731226326, 0.027447439849105214,\n",
    "        0.013256007422060696, 0.01155863274600911, 0.004522818236187147, 0.0045065249727806655,\n",
    "        0.004329215929827789, 0.003943928171627486, 0.0029251199950928526, 0.0027363098250295197,\n",
    "        0.0023232276763122785, 0.0022705141770560182, 0.0018171780834521783, 0.0015440263145788287,\n",
    "        0.001292918372667188, 0.0010964407845302172, 0.0009066721872076797, 0.000854917115210624,\n",
    "        0.0006613148088512674, 0.0004907147567128245, 5.750563555228412e-06, 4.792136296023677e-06\n",
    "      ]\n",
    "    },\n",
    "    \"values_column\": \"lab_value\",\n",
    "    \"_measurement_metadata\": \"/home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/inferred_measurement_metadata/lab_name.csv\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Here, in addition to the same information we see for eye color, we also see listed the associated values\n",
    "column for this multivariate regression, and also a path to a measurement metadata object that contains more\n",
    "statistics for this measurement. In addition, we can also see that under this configuration, the system has\n",
    "expanded the two laboratory tests `'SOFA'` and `'GCS'` into categorical options.\n",
    "\n",
    "We can inspect the detailed measurement metadata linked in this config object by looking at the csv file in\n",
    "question.\n",
    "\n",
    "```{literalinclude} ../../sample_data/processed/sample/inferred_measurement_metadata/lab_name.csv\n",
    "---\n",
    "language: csv\n",
    "---\n",
    "```\n",
    "\n",
    "Within this file, we see a dataframe containing information about the different laboratory tests that the\n",
    "system has processed, including their value type, information about the learned outlier model, and information\n",
    "learned about their normalization variables. For example, the system has inferred that the GCS and SOFA scores\n",
    "are categorical, integer variables, the SpO2 lab is an integer variable, and the potassium and creatinine labs\n",
    "are floating point labs. Further, it has learned outlier bounds for the various continuous laboratory tests\n",
    "and has fit the mean and standard deviation of the laboratory test values for these as well.\n",
    "\n",
    "##### Processed DataFrames\n",
    "\n",
    "###### `subjects_df.parquet`, `events_df.parquet`, `dynamic_measurements_df.parquet`\n",
    "\n",
    "These files are the output, processed, internally represented versions of the raw input data, organized\n",
    "according to the event-stream data model (see the Usage Guide for more information on that data model). \n",
    "\n",
    "###### `DL_reps`\n",
    "\n",
    "This directory contains the deep-learning formatted representtaions of the data. It is suitable for rapidly\n",
    "iterating through batches of subject time-series, but less well suited towards querying and data manipulation.\n",
    "\n",
    "```bash\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example+ ± ls DL_reps/\n",
    "held_out_0.parquet train_0.parquet tuning_0.parquet\n",
    "```\n",
    "\n",
    "##### Overall Class File\n",
    "\n",
    "###### `E.pkl`\n",
    "\n",
    "This file contains the class object itself and a collection of its attributes. It, importantly, _does not_\n",
    "contain the nested dataframes (`subjects_df`, `events_df`, `dynamic_measurements_df`), as these are stored in\n",
    "the files mentioned above and loaded lazily by the object during use, and similarly does not store the learned\n",
    "measurement metadata files also mentioned above, which are lazily loaded in the same way.\n",
    "\n",
    "```{warning}\n",
    "Currently, this lazy saving/loading uses absolute paths, which makes transferring datasets to new locations more challenging! Paths can be modified in the class and config object locally to fix this problem, but it is a challenge.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e500e9f",
   "metadata": {},
   "source": [
    "## Inspecting the Synthetic Dataset Produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78458ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a738dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "\n",
    "pl.Config.set_tbl_cols(7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcd768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"processed/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25b21b",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6735b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de2e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/subjects_df.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subject_id</th><th>MRN</th><th>eye_color</th><th>dob</th></tr><tr><td>u8</td><td>cat</td><td>cat</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>0</td><td>&quot;310243&quot;</td><td>&quot;GREEN&quot;</td><td>1981-07-28 00:00:00</td></tr><tr><td>1</td><td>&quot;384198&quot;</td><td>&quot;BROWN&quot;</td><td>1985-04-15 00:00:00</td></tr><tr><td>2</td><td>&quot;520533&quot;</td><td>&quot;BROWN&quot;</td><td>1979-04-15 00:00:00</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌────────────┬────────┬───────────┬─────────────────────┐\n",
       "│ subject_id ┆ MRN    ┆ eye_color ┆ dob                 │\n",
       "│ ---        ┆ ---    ┆ ---       ┆ ---                 │\n",
       "│ u8         ┆ cat    ┆ cat       ┆ datetime[μs]        │\n",
       "╞════════════╪════════╪═══════════╪═════════════════════╡\n",
       "│ 0          ┆ 310243 ┆ GREEN     ┆ 1981-07-28 00:00:00 │\n",
       "│ 1          ┆ 384198 ┆ BROWN     ┆ 1985-04-15 00:00:00 │\n",
       "│ 2          ┆ 520533 ┆ BROWN     ┆ 1979-04-15 00:00:00 │\n",
       "└────────────┴────────┴───────────┴─────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESD.subjects_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee61e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading events from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/events_df.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>subject_id</th><th>timestamp</th><th>event_type</th><th>age</th><th>age_is_inlier</th></tr><tr><td>u32</td><td>u8</td><td>datetime[μs]</td><td>cat</td><td>f64</td><td>bool</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>2010-06-24 13:23:00</td><td>&quot;ADMISSION&amp;VITA…</td><td>-0.540896</td><td>true</td></tr><tr><td>1</td><td>0</td><td>2010-06-24 14:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.540871</td><td>true</td></tr><tr><td>2</td><td>0</td><td>2010-06-24 15:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.540845</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 6)\n",
       "┌──────────┬────────────┬─────────────────────┬─────────────────────┬───────────┬───────────────┐\n",
       "│ event_id ┆ subject_id ┆ timestamp           ┆ event_type          ┆ age       ┆ age_is_inlier │\n",
       "│ ---      ┆ ---        ┆ ---                 ┆ ---                 ┆ ---       ┆ ---           │\n",
       "│ u32      ┆ u8         ┆ datetime[μs]        ┆ cat                 ┆ f64       ┆ bool          │\n",
       "╞══════════╪════════════╪═════════════════════╪═════════════════════╪═══════════╪═══════════════╡\n",
       "│ 0        ┆ 0          ┆ 2010-06-24 13:23:00 ┆ ADMISSION&VITAL&LAB ┆ -0.540896 ┆ true          │\n",
       "│ 1        ┆ 0          ┆ 2010-06-24 14:23:00 ┆ VITAL&LAB           ┆ -0.540871 ┆ true          │\n",
       "│ 2        ┆ 0          ┆ 2010-06-24 15:23:00 ┆ VITAL&LAB           ┆ -0.540845 ┆ true          │\n",
       "└──────────┴────────────┴─────────────────────┴─────────────────────┴───────────┴───────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESD.events_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a8c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dynamic_measurements from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/dynamic_measurements_df.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>measurement_id</th><th>department</th><th>HR</th><th>&hellip;</th><th>HR_is_inlier</th><th>temp_is_inlier</th><th>lab_name_is_inlier</th></tr><tr><td>u32</td><td>cat</td><td>f64</td><td>&hellip;</td><td>bool</td><td>bool</td><td>bool</td></tr></thead><tbody><tr><td>189</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1995</td><td>null</td><td>-0.76645</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>2126</td><td>null</td><td>-0.89035</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>23325</td><td>null</td><td>-0.745154</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>36909</td><td>null</td><td>-0.996826</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>60904</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>67018</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>301250</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>925618</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>1082040</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 10)\n",
       "┌────────────┬────────────┬───────────┬──────────┬───┬───────────────┬──────────────┬──────────────┐\n",
       "│ measuremen ┆ department ┆ HR        ┆ temp     ┆ … ┆ HR_is_inlier  ┆ temp_is_inli ┆ lab_name_is_ │\n",
       "│ t_id       ┆ ---        ┆ ---       ┆ ---      ┆   ┆ ---           ┆ er           ┆ inlier       │\n",
       "│ ---        ┆ cat        ┆ f64       ┆ f64      ┆   ┆ bool          ┆ ---          ┆ ---          │\n",
       "│ u32        ┆            ┆           ┆          ┆   ┆               ┆ bool         ┆ bool         │\n",
       "╞════════════╪════════════╪═══════════╪══════════╪═══╪═══════════════╪══════════════╪══════════════╡\n",
       "│ 189        ┆ PULMONARY  ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ null         │\n",
       "│ 1995       ┆ null       ┆ -0.76645  ┆ 1.013012 ┆ … ┆ true          ┆ true         ┆ null         │\n",
       "│ 2126       ┆ null       ┆ -0.89035  ┆ 0.753025 ┆ … ┆ true          ┆ true         ┆ null         │\n",
       "│ 23325      ┆ null       ┆ -0.745154 ┆ 0.88302  ┆ … ┆ true          ┆ true         ┆ null         │\n",
       "│ …          ┆ …          ┆ …         ┆ …        ┆ … ┆ …             ┆ …            ┆ …            │\n",
       "│ 67018      ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "│ 301250     ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "│ 925618     ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "│ 1082040    ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "└────────────┴────────────┴───────────┴──────────┴───┴───────────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESD.dynamic_measurements_df.filter(pl.col(\"event_id\") == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9a8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
