{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2a71f",
   "metadata": {},
   "source": [
    "# Local Data Tutorial\n",
    "In this tutorial, rather than running real models and configurations over MIMIC-IV, we'll work with a set of\n",
    "local, synthetic files distributed with this repository, with the goal being to fully explore the details of\n",
    "this pipeline. This tutorial will consist of both content on this page, running certain scripts on one's local\n",
    "machine, and some jupyter notebooks. We will walk through the entire pipeline with these local examples and\n",
    "discuss limitations of the pipeline, details of classes, scripts, etc.\n",
    "\n",
    "## Synthetic Data\n",
    "For this tutorial, we'll use the three synthetic data files distributed in the [sample_data/raw](<>) folder in\n",
    "the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b07a006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 55M\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 12:42 \u001b[0m\u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 5 mmd mmd 4.0K Jul 17 11:42 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 3.6M Jul 17 11:40 admit_vitals.csv\r\n",
      "-rw-rw-r-- 1 mmd mmd  52M Jul 17 11:40 labs.csv\r\n",
      "-rw-rw-r-- 1 mmd mmd 4.2K Jul 17 11:40 subjects.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls --color -lah raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41de5b",
   "metadata": {},
   "source": [
    "```{note}\n",
    "To see how those files are generated, look at [sample_data/generate_synthetic_data.ipynb]().\n",
    "```\n",
    "\n",
    "These files contain the following data:\n",
    "\n",
    "### `subjects.csv`\n",
    "\n",
    "This file contains per-subject data. It has one row per subject, with each row containing a subject identifier (here called \"`MRN`\"), a date of birth (\"`dob`\"), the subject's eye color (`eye_color`), and the subject's height (\"`height`\"):\n",
    "\n",
    "```{note}\n",
    "To examine these csv files locally, we use the `csvkit` unix command line package. You can learn more about it [here](https://csvkit.readthedocs.io/en/latest/index.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99ad356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     MRN |        dob | eye_color |   height |\r\n",
      "| ------- | ---------- | --------- | -------- |\r\n",
      "| 310,243 | 1981-07-28 | GREEN     | 178.768… |\r\n",
      "| 384,198 | 1985-04-15 | BROWN     | 168.319… |\r\n",
      "| 520,533 | 1979-04-15 | BROWN     | 165.836… |\r\n",
      "| 850,710 | 1970-08-08 | HAZEL     | 159.722… |\r\n",
      "| 379,433 | 1971-06-25 | BLUE      | 153.749… |\r\n",
      "|     ... |        ... | ...       |      ... |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" --max-rows=5 raw/subjects.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863c2c8",
   "metadata": {},
   "source": [
    "### `admit_vitals.csv`\n",
    "\n",
    "This file contains dynamic data quantifying both fictional subject hospital admissions, and fictional vitals signs measured for those subjects. Each row of this file records a unique vitals sign measurement for a patient, affiliated with the associated admission listed in the row. This means that admission level information is _heavily duplicated_ within this file, which is a phenomena sometimes observed in real data, and something we'll need to account for in our pipeline's setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7878a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       MRN | admit_date           | disch_date           | department | vitals_date          |    HR |  temp |\r\n",
      "| --------- | -------------------- | -------------------- | ---------- | -------------------- | ----- | ----- |\r\n",
      "| 1,549,363 | 01/04/2010, 06:36:31 | 01/14/2010, 11:41:31 | ORTHOPEDIC | 01/11/2010, 14:18:35 |  77.1 |  96.3 |\r\n",
      "|   415,881 | 02/11/2010, 04:59:07 | 02/14/2010, 07:56:23 | ORTHOPEDIC | 02/11/2010, 10:34:17 | 148.5 |  95.6 |\r\n",
      "|    42,335 | 03/06/2010, 05:33:18 | 03/16/2010, 05:09:11 | CARDIAC    | 03/13/2010, 10:47:39 |  46.7 | 101.0 |\r\n",
      "| 1,516,810 | 02/11/2010, 23:23:26 | 02/22/2010, 23:59:04 | CARDIAC    | 02/12/2010, 16:58:44 |  94.2 |  95.2 |\r\n",
      "|   928,262 | 02/19/2010, 03:51:12 | 02/20/2010, 23:32:31 | CARDIAC    | 02/19/2010, 18:08:30 |  77.3 |  99.7 |\r\n",
      "|       ... | ...                  | ...                  | ...        | ...                  |   ... |   ... |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" --max-rows=5 raw/admit_vitals.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b481523",
   "metadata": {},
   "source": [
    "### `labs.csv`\n",
    "\n",
    "This file contains dynamic data quantifying fictional subject laboratory test measurements. Each row of this file contains a record of a particular lab test measured for a subject. Note that the lab data is not organized into separate columns for each lab; rather each row contains a pair of a lab test name and the associated value; this is what we call in ESGPT a \"multivariate regression\" column encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d05a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     MRN | timestamp           | lab_name   | lab_value |\r\n",
      "| ------- | ------------------- | ---------- | --------- |\r\n",
      "| 928,262 | 09:31:08-2010-03-04 | SpO2       |     50.00 |\r\n",
      "| 689,012 | 04:28:32-2010-08-18 | creatinine |      1.45 |\r\n",
      "| 706,423 | 01:18:05-2010-07-22 | SpO2       |     51.00 |\r\n",
      "| 383,358 | 16:00:02-2010-05-07 | SpO2       |     55.00 |\r\n",
      "| 407,452 | 15:36:06-2010-09-27 | SpO2       |     50.00 |\r\n",
      "|     ... | ...                 | ...        |       ... |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" --max-rows=5 raw/labs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcd527",
   "metadata": {},
   "source": [
    "# Processing Synthetic Data with ESGPT\n",
    "\n",
    "Now that we see the form of this synthetic data, we can examine how to process it with Event Stream GPT. From\n",
    "the base directory of the ESGPT repository, we can run the following command:\n",
    "\n",
    "```bash\n",
    "PYTHONPATH=$(pwd):$PYTHONPATH ./scripts/build_dataset.py \\\n",
    "\t--config-path=\"$(pwd)/sample_data/\" \\\n",
    "\t--config-name=dataset \\\n",
    "\t\"hydra.searchpath=[$(pwd)/configs]\"\n",
    "```\n",
    "\n",
    "```{note}\n",
    "This script, like all built-in ESGPT scripts, uses [Hydra](https://hydra.cc/), a configuration file and experiment run-script library. In hydra, all scripts can take as input a set of composable configuration files which can be overwritten via files or viat he command line. If you aren't already familiar with Hydra, you should read through some of their examples or tutorials to gain some familiarity with their system.\n",
    "```\n",
    "\n",
    "Before we actually run this command, we need to do 2 things:\n",
    "  1. Decide what we _want_ the command to do, conceptually.\n",
    "  2. Understand what we're _telling_ the library to do, via its input arguments.\n",
    "  \n",
    "## What do we _want_ to happen?\n",
    "We can see that our synthetic data has a few different kinds of things happening to these subjects. In the ESGPT data model, we want to organize this data so that we clearly know who our subjects are, quantify when things happen to those subjects, and record in a sparse manner what is happening to those patients. Let's list a few more specific desiderata:\n",
    "  1. We should expect our system to quantify those subjects in our synthetic data that meet our inclusion criteria (which we haven't yet specified).\n",
    "  2. The system should bucket all interactions for subjects into appropriately defined events, across admissions, discharges, vitals signs, and laboratory tests.\n",
    "  3. The system should learn appropriate categorical vocabularies, numerical outlier detector models, numerical normalization models, for the various measurements we want to extract (which we haven't yet specified).\n",
    "  4. The system should produce \"deep-learning friendly\" representations of these data.\n",
    "\n",
    "```{note}\n",
    "A quick tangent -- what do we mean by \"deep-learning friendly\" representations of these data? Well, right now, if we were to try to run these data through _any_ deep-learning system for longitudinal data, we'd need to re-format these data such that it is easy to efficiently (ideally $O(1)$) retrieve all data corresponding to a single subject in an organized timeseries format that we can then efficiently (meaning in a manner requiring minimal GPU memory) pass into a sequential neural network. \n",
    "\n",
    "In the current representation, this retrieval process would not be $O(1)$; instead, if we didn't modify the data's organization at all, for each new MRN, we'd need to select from each data file all those rows with that MRN (each selection being an $O(N)$ operation), and then we would need to subsequently sort all the temporal data by timestamp (another $O(L\\ln(L))$ operation).\n",
    "\n",
    "Similarly, if we use a naive, dense encoding of the data per measurement for our DL representation, this will be very wasteful in terms of GPU memory, as each record will need to occupy memory proportionate to the total number of possible measurements we could observe in our data (e.g., the total number of lab tests, plus the total number of vitals signs, plus the total number of admission departments, etc.). Instead, a sparse encoding should be used.\n",
    "\n",
    "These two properties are exactly what we mean by a \"deep-learning friendly\" representation of the data.\n",
    "```\n",
    "\n",
    "We can see that there are several questions posed by these desiderata that we need to answer, such as:\n",
    "  1. What are our inclusion criteria?\n",
    "  2. How should we bucket interactions into events?\n",
    "  3. What measurements do we want to extract?\n",
    "  4. How do we want to define \"outliers\"?\n",
    "  5. How do we define \"appropriate categorical vocabularies\"?\n",
    "  6. How do we want to normalize numerical measurements?\n",
    "  \n",
    "To start us off, let's use the following answers:\n",
    "  1. We'll include all subjects who have at least 3 events, with no other inclusion/exclusion criteria.\n",
    "  2. We'll define an \"event\" to be any interactions happening to a patient within a 1 hour period. We'll bucket these interactions together starting at the earliest event.\n",
    "  3. Ideally, we'd like to extract _all_ measurements. As we'll see, however, due to a limitation in the current version of ESGPT, we'll extract all measurements except for the patient's height. In particular, we'll extract the occurrence of admissions, discharges, vitals signs, and laboratory tests, as well as the subject's age, eye color,  admission department, the values recorded for HR and temperature, and all lab test values.\n",
    "  4. We'll use a very simple outlier model, that excludes numerical data as outliers if their values exceed 1.5 standard deviations from the mean. This is an extremely aggressive cutoff only suitable for this synthetic data setting.\n",
    "  5. We'll keep any categorical observation as a vocabulary element if it occurs at least 5 times.\n",
    "  6. We'll normalize our numerical observations to have zero mean and unit variance.\n",
    "  \n",
    "## Telling the pipeline what to do: input config\n",
    "Now that we have some basic idea of what we want the pipeline to do, let's examine the input configuration file that we pass to the dataset script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c11c506",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\r\n",
      "  - dataset_base\r\n",
      "  - _self_\r\n",
      "\r\n",
      "# So that it can be run multiple times without issue.\r\n",
      "do_overwrite: True\r\n",
      "\r\n",
      "cohort_name: \"sample\"\r\n",
      "subject_id_col: \"MRN\"\r\n",
      "raw_data_dir: \"./sample_data/raw/\"\r\n",
      "save_dir: \"./sample_data/processed/${cohort_name}\"\r\n",
      "\r\n",
      "DL_chunk_size: null\r\n",
      "\r\n",
      "inputs:\r\n",
      "  subjects:\r\n",
      "    input_df: \"${raw_data_dir}/subjects.csv\"\r\n",
      "  admissions:\r\n",
      "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\r\n",
      "    start_ts_col: \"admit_date\"\r\n",
      "    end_ts_col: \"disch_date\"\r\n",
      "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\r\n",
      "    event_type: [\"OUTPATIENT_VISIT\", \"ADMISSION\", \"DISCHARGE\"]\r\n",
      "  vitals:\r\n",
      "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\r\n",
      "    ts_col: \"vitals_date\"\r\n",
      "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\r\n",
      "  labs:\r\n",
      "    input_df: \"${raw_data_dir}/labs.csv\"\r\n",
      "    ts_col: \"timestamp\"\r\n",
      "    ts_format: \"%H:%M:%S-%Y-%m-%d\"\r\n",
      "\r\n",
      "measurements:\r\n",
      "  static:\r\n",
      "    single_label_classification:\r\n",
      "      subjects: [\"eye_color\"]\r\n",
      "  functional_time_dependent:\r\n",
      "    age:\r\n",
      "      functor: AgeFunctor\r\n",
      "      necessary_static_measurements: { \"dob\": [\"timestamp\", \"%m/%d/%Y\"] }\r\n",
      "      kwargs: { dob_col: \"dob\" }\r\n",
      "  dynamic:\r\n",
      "    multi_label_classification:\r\n",
      "      admissions: [\"department\"]\r\n",
      "    univariate_regression:\r\n",
      "      vitals: [\"HR\", \"temp\"]\r\n",
      "    multivariate_regression:\r\n",
      "      labs: [[\"lab_name\", \"lab_value\"]]\r\n",
      "\r\n",
      "outlier_detector_config:\r\n",
      "  stddev_cutoff: 1.5\r\n",
      "min_valid_vocab_element_observations: 5\r\n",
      "min_valid_column_observations: 5\r\n",
      "min_true_float_frequency: 0.1\r\n",
      "min_unique_numerical_observations: 20\r\n",
      "min_events_per_subject: 3\r\n",
      "agg_by_time_scale: \"1h\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat dataset.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ddfa1",
   "metadata": {},
   "source": [
    "There are a number of sections in this file. Firstly, the first three lines ensure this config builds on the defaults provided with the ESGPT library, via Hydra's normal mechanisms. If you aren't familiar with this syntax, check out the [Hydra documentation](https://hydra.cc/docs/1.3/advanced/defaults_list/).\n",
    "\n",
    "Next, there is a section defining some overarching variables and a section defining our input sources. We can see this section details the paths to each of our input files as well as the formatting used for (most of) the timestamps within these files. Note that this section makes use of [Hydra/OmegaConf's Interpolations](https://omegaconf.readthedocs.io/en/2.3_branch/grammar.html#interpolation-strings) to simplify the specification of the file paths used. \n",
    "\n",
    "```{warning}\n",
    "Two parameters in this section are required: `subject_id_col`, and `cohort_name`. This will be explored in more detail later in this tutorial.\n",
    "```\n",
    "\n",
    "Next, we have a section defining the various measurements we'll exctract in this dataset. We can see we specify each of the measurements we discussed above:\n",
    "  1. `eye_color` is extracted as a `static`, `single_label_classification` measure. \n",
    "  2. `age` is extracted as a `functional_time_dependent` measure, leveraging the date-of-birth column `dob`. _Note that this is where we define the timestamp format for the `dob` column, as it is a timestamp formatted static column!_\n",
    "  3. `department` is extracted as a `dynamic`, `multi_label_classification` measure.\n",
    "  4. `HR`, and `temp` are extracted as `dynamic`, `univariate_regression` measures.\n",
    "  5. `lab_name` and `lab_value` are extracted as a single `dynamic`, `multivariate_regression` measure.\n",
    "  \n",
    "```{note}\n",
    "The terms `static`, `functional_time_dependent`, & `dynamic` and `single_label_classification`, `multi_label_classification`, `univariate_regression`, and `multivariate_regression`, are defined enumerations in the `EventStream.data.config` sub-module, and dictate where measurements are stored and how they are pre-processed.\n",
    "```\n",
    "  \n",
    "Finally, we have the remaining set of parameters, which define our inclusion-exclusion criteria (by specifying `min_events_per_subject`), our outlier and normalizer model configuration parameters (`normalization` being omitted here as what we want is the default value), our filtering thresholds for vocabulary elements, and the aggregation time-scale for events.\n",
    "\n",
    "### What else _could_ we have specified?\n",
    "To better understand the structure of this input specification, let's explore this input configuration file in a bit more detail. To start with, let's look at what the default, base config contains (the config we inherit from in the defaults list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34bed35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\r\n",
      "  - outlier_detector_config: stddev_cutoff\r\n",
      "  - normalizer_config: standard_scaler\r\n",
      "  - _self_\r\n",
      "\r\n",
      "cohort_name: ???\r\n",
      "save_dir: ${oc.env:PROJECT_DIR}/data/${cohort_name}\r\n",
      "subject_id_col: ???\r\n",
      "seed: 1\r\n",
      "split: [0.8, 0.1]\r\n",
      "do_overwrite: False\r\n",
      "DL_chunk_size: 20000\r\n",
      "min_valid_vocab_element_observations: 25\r\n",
      "min_valid_column_observations: 50\r\n",
      "min_true_float_frequency: 0.1\r\n",
      "min_unique_numerical_observations: 25\r\n",
      "min_events_per_subject: 20\r\n",
      "agg_by_time_scale: null\r\n",
      "\r\n",
      "hydra:\r\n",
      "  job:\r\n",
      "    name: build_${cohort_name}\r\n",
      "  run:\r\n",
      "    dir: ${save_dir}/.logs\r\n",
      "  sweep:\r\n",
      "    dir: ${save_dir}/.logs\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../configs/dataset_base.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eb1cd",
   "metadata": {},
   "source": [
    "We can see there are some parameters we're familiar with and some we're not. Firstly, we can see that this default base config marks `cohort_name` and `subject_id_col` with `???`. This is the OmegaConf provided value to represent a value that _needs to be overwritten_ in downstream usage. This is why those two parameters are mandatory. This config also has variables for the seed, split size, and some hydra-internal parameters. Further, it points to two  further default configs for the outlier detector and normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b5f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls: stddev_cutoff\r\n",
      "stddev_cutoff: 5.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../configs/outlier_detector_config/stddev_cutoff.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723c10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls: standard_scaler\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../configs/normalizer_config/standard_scaler.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0888ce",
   "metadata": {},
   "source": [
    "These are both quite simple, but show how the final config will be constructed from these values. \n",
    "\n",
    "One thing that is notably missing from this broader structure is any notion of included `inputs` or `measurements` sections. To understand how we can further specify our config, we need to understand how we could modify those sections as well.\n",
    "\n",
    "#### Inputs\n",
    "This section allows us to specify which input data frames should be read, and from where. The `inputs:` option should be an object whose keys are the names of input sources and whose values are configuration for those inputs. Currently, two input formats are possible:\n",
    "  1. The `input_df` format, which is used in this synthetic example. This format has an input configuration that contains the `input_df:` key whose value is a file path pointing to a `csv` or `parquet` data-frame file on disk that contains that input source's data. For example:\n",
    "  ```yaml\n",
    "admissions:\n",
    "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\n",
    "    start_ts_col: \"admit_date\"\n",
    "    end_ts_col: \"disch_date\"\n",
    "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\n",
    "    event_type: [\"OUTPATIENT_VISIT\", \"ADMISSION\", \"DISCHARGE\"]\n",
    "  ```\n",
    "  2. The `query` format, which is used in the MIMIC-IV tutorial. In this format, you must specify a `query` parameter. This parameter can either be a string query or a list of string queries, in which case you must specify a global `connection_uri` parameter detailing the URI of the database to which you wish to query (In the [connector-x](https://github.com/sfu-db/connector-x) format), or a dictionary, with keys and values specifying parameters of the [`EventStream.data.dataset_polars.Query`]() object. For example:\n",
    "  ```yaml\n",
    "patients:\n",
    "    query: |-\n",
    "      SELECT subject_id, gender, to_date((anchor_year-anchor_age)::CHAR(4), 'YYYY') AS year_of_birth\n",
    "      FROM mimiciv_hosp.patients\n",
    "      WHERE subject_id IN (\n",
    "        SELECT long_icu.subject_id FROM (\n",
    "          (\n",
    "            SELECT subject_id FROM mimiciv_icu.icustays WHERE los > ${min_los}\n",
    "          ) AS long_icu INNER JOIN (\n",
    "            SELECT subject_id\n",
    "            FROM mimiciv_hosp.admissions\n",
    "            GROUP BY subject_id\n",
    "            HAVING COUNT(*) > ${min_admissions}\n",
    "          ) AS many_admissions\n",
    "          ON long_icu.subject_id = many_admissions.subject_id\n",
    "        )\n",
    "      )\n",
    "    must_have: [\"gender\", \"year_of_birth\"]\n",
    "  ```\n",
    "  \n",
    "Each input can also have a number of other keys and values, including:\n",
    "\n",
    "##### Timestamp & Event-type Specification.\n",
    "For non-static data sources, the keys `ts_col` or `start_ts_col` and `end_ts_col` specify the name of the column (or columns) containing the timestamp for the event, and `ts_format` the format of that timestamp. `ts_col` is used for data-sources where each row represents one event, and `start_`/`end_ts_col` for data-sources where each row specifies a range in time. For example, in our synthetic config,\n",
    "```yaml\n",
    "admissions:\n",
    "    input_df: \"${raw_data_dir}/admit_vitals.csv\"\n",
    "    start_ts_col: \"admit_date\"\n",
    "    end_ts_col: \"disch_date\"\n",
    "    ts_format: \"%m/%d/%Y, %H:%M:%S\"\n",
    "    event_type: [\"OUTPATIENT_VISIT\", \"ADMISSION\", \"DISCHARGE\"]\n",
    "```\n",
    "\n",
    "specifies a range event, where the start timestamp is stored in `admit_date` and the end timestamp in `disch_date`, formatted as `\"%m/%d/%Y, %H:%M:%S\"`. In contrast,\n",
    "\n",
    "```yaml\n",
    "labs:\n",
    "    input_df: \"${raw_data_dir}/labs.csv\"\n",
    "    ts_col: \"timestamp\"\n",
    "    ts_format: \"%H:%M:%S-%Y-%m-%d\"\n",
    "```\n",
    "\n",
    "captures data where each row is a single-timepoint event, with timestamp stored in `\"%H:%M:%S-%Y-%m-%d\"` format in column `timestamp`.\n",
    "\n",
    "You can also explicitly set the type of each event. Events' types in ESGPT are categorical variables defined by the user that are used to dictate any intra-event causal dependency graphs in downstream models, can be used to help define downstream tasks, and are otherwise used to analyze and describe data. When using the pre-defined build dataset script, they can either be explicitly set or are automatically inferred from the name of the input block. For example, in the examples given above, the `labs:` block produces an input source with the event type `LAB` (the singular, upper-cased inflection of the name of the block, `'labs'`), and `admissions` (being a range event) produces events of type `'OUTPATIENT_VISIT'` when `admit_date == disch_date` and `'ADMISSION'` on `admit_date` and `'DISCHARGE'` on `disch_date`.\n",
    "```{note}\n",
    "For range events, the default event types are defined to be `*_EQ`, `*_START`, and `*_END`, where `*` is the singular, upper-cased inflection of the input block name.\n",
    "```\n",
    "\n",
    "Event types can also be defined to be column dependent. For example, in this config example (which is not part of our current synthetic example config), we see that event types are defined to take on the value of the column `'visit_occurrence_concept_name'` for the case that the start and end times are the same and for start events, but the static `'Drug Stop'` for end events.\n",
    "\n",
    "```yaml\n",
    "drugs:\n",
    "    input_df: \"${raw_data_dir}/drug.parquet\"\n",
    "    start_ts_col: \"drug_exposure_start_datetime\"\n",
    "    end_ts_col: [\"drug_exposure_end_datetime\", \"verbatim_end_date\"]\n",
    "    event_type: [\"COL:visit_occurrence_concept_name\", \"COL:visit_occurrence_concept_name\", \"Drug Stop\"]\n",
    "    start_columns: {\"standard_concept_name\": \"drug\", \"drug_type_concept_name\": \"drug_type\"}\n",
    "    end_columns: \n",
    "        standard_concept_name: drug\n",
    "        drug_type_concept_name: drug_type\n",
    "        stop_reason: drug_stop_reason\n",
    "```\n",
    "\n",
    "##### Filtering\n",
    "You can also specify a simple filter used for a given input source. For example, in the `patients` block in the MIMIC-IV example, we specify that valid rows must have `'gender'` and `'year_of_birth'` defined and non-null. This is another way to enforce cohort inclusion/exclusion criteria. The filter object can either be a list of strings, in which case those columns must have non-null values, or a dictionary from column names to either the boolean `True` (indicating the column must be present and non-null) or lists of allowable values for that column.\n",
    "\n",
    "##### Measurement columns to extract\n",
    "You can also specify which measurements should be extracted to associate with a given input data source. Largely, this information will be determined automatically based on the `measurements` section of the config; however, it can be specified explicitly as well. The most common case this would be done is to differentiate different measurements to associate with `start` and `end` events for range events or to re-name measurements from their input column names to new names for internal use (this can be done not only for cosmetic reasons, but so as to unify or disentangle measurements across different input files). For example, in the `drugs:` example shown above, the columns `standard_concept_name` and `drug_type_concept_name` are both used for both start and end events, and are renamed to `'drug'`, and `'drug_type'` in both cases, whereas `stop_reason` is used only for end events (and is renamed to `drug_stop_reason`).\n",
    "#### Measurements Section\n",
    "The `measurements:` block lists all the actual measurements that should be extracted from those input sources, broken down into categories based on their `temporality` and `modality` (see `EventStream.data.types.TemporalityType` and `EventStream.data.types.DataModality`, respectively). \n",
    "\n",
    "The only non-standard portion of this block corresponds to the `functional_time_dependent` block, which specifies measurements whose values are _not_ stored in the raw input data by default, but are instead computable dynamically given per-subject static data and the timestamps of other events that occur in the data. A good example is a subject's age, which is included in our synthetic configuration. Given a subject's date-of-birth and the timestamp of any other event, we can dynamically compute the subject's age as of that event, which is exactly what the `EventStream.data.time_dependent_functor.AgeFunctor` does.\n",
    "\n",
    "The structure of this config section is\n",
    "```yaml\n",
    "functional_time_dependent:\n",
    "  output_measurement_name:\n",
    "      functor: ??? # The functor that is used for this measurement. Must be in `EventStream.data.config.MeasurementConfig.FUNCTORS`\n",
    "      necessary_static_measurements: { \"static_measurement_column\": ??? } # column name: column formatting info\n",
    "      kwargs: { kwarg: kwval } # Keyword args to pass to functor constructor.\n",
    "```\n",
    "\n",
    "Currently, only [`AgeFunctor`]() and [`TimeOfDayFunctor`] are pre-defined and supported, but this can be extended by the user by directly adding new functors to the [`EventStream.data.config.MeasurementConfig`]() object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d861ab8",
   "metadata": {},
   "source": [
    "## Running the Command\n",
    "Now that we understand the setup a bit better, let's run the actual command:\n",
    "\n",
    "```bash\n",
    "PYTHONPATH=$(pwd):$PYTHONPATH ./scripts/build_dataset.py \\\n",
    "\t--config-path=\"$(pwd)/sample_data/\" \\\n",
    "\t--config-name=dataset \\\n",
    "\t\"hydra.searchpath=[$(pwd)/configs]\"\n",
    "```\n",
    "\n",
    "You should see as output the printed line `Empty new events dataframe of type OUTPATIENT_VISIT!`, but\n",
    "otherwise nothing. Before we proceed further, let's break down what this process has done, and how it could do\n",
    "things differently. \n",
    "\n",
    "Firstly, let's take a look at what is produced in the output folder itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad4bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30M\tprocessed/sample/\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh processed/sample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1ffa37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed/sample:\r\n",
      "total 19M\r\n",
      "drwxrwxr-x 5 mmd mmd 4.0K Jul 14 17:14 \u001b[0m\u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.8K Jul 17 11:41 config.json\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34mDL_reps\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd  12M Jul 17 11:41 dynamic_measurements_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 5.2K Jul 17 11:41 E.pkl\r\n",
      "-rw-rw-r-- 1 mmd mmd 7.1M Jul 17 11:41 events_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.5K Jul 17 11:40 hydra_config.yaml\r\n",
      "-rw-rw-r-- 1 mmd mmd 2.9K Jul 17 11:41 inferred_measurement_configs.json\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34minferred_measurement_metadata\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.7K Jul 17 11:40 input_schema.json\r\n",
      "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m.logs\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 2.7K Jul 17 11:41 subjects_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd  772 Jul 17 11:41 vocabulary_config.json\r\n",
      "\r\n",
      "processed/sample/DL_reps:\r\n",
      "total 12M\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 5 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.2M Jul 17 11:41 held_out_0.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 8.9M Jul 17 11:41 train_0.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.2M Jul 17 11:41 tuning_0.parquet\r\n",
      "\r\n",
      "processed/sample/inferred_measurement_metadata:\r\n",
      "total 24K\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 5 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd  181 Jul 17 11:41 age.csv\r\n",
      "-rw-rw-r-- 1 mmd mmd  181 Jul 17 11:41 HR.csv\r\n",
      "-rw-rw-r-- 1 mmd mmd  713 Jul 17 11:41 lab_name.csv\r\n",
      "-rw-rw-r-- 1 mmd mmd  181 Jul 17 11:41 temp.csv\r\n",
      "\r\n",
      "processed/sample/.logs:\r\n",
      "total 12K\r\n",
      "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 5 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd    0 Jul 14 17:14 build_sample.log\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m.hydra\u001b[0m\r\n",
      "\r\n",
      "processed/sample/.logs/.hydra:\r\n",
      "total 20K\r\n",
      "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m.\u001b[0m\r\n",
      "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 17:14 \u001b[01;34m..\u001b[0m\r\n",
      "-rw-rw-r-- 1 mmd mmd 1.5K Jul 17 11:40 config.yaml\r\n",
      "-rw-rw-r-- 1 mmd mmd 3.5K Jul 17 11:40 hydra.yaml\r\n",
      "-rw-rw-r-- 1 mmd mmd    3 Jul 17 11:40 overrides.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls --color -lah -R processed/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a44939",
   "metadata": {},
   "source": [
    "Now, let's walk through what happens when we run this script, step-by-step, and how each of the files listed above are produced.\n",
    "\n",
    "### Step 1: Config Parsing\n",
    "First, the script parses our input config file into a slightly refined structured form, then passes that as input to the `EventStream.data.dataset_polars.Dataset` constructor.\n",
    "\n",
    "To see what this process looks like, we can inspect one portion of the output of the overall script, which we can find in the [sample_data/processed/sample]() directory; in particular, the `input_schema.json` file.\n",
    "```{note}\n",
    "The `sample_data/processed/sample` directory is the `save_dir` key in our `dataset.yaml` configuration file.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6db36770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"static\": {\r\n",
      "        \"input_df\": \"./sample_data/raw//subjects.csv\",\r\n",
      "        \"type\": \"static\",\r\n",
      "        \"event_type\": null,\r\n",
      "        \"subject_id_col\": \"MRN\",\r\n",
      "        \"ts_col\": null,\r\n",
      "        \"start_ts_col\": null,\r\n",
      "        \"end_ts_col\": null,\r\n",
      "        \"ts_format\": null,\r\n",
      "        \"start_ts_format\": null,\r\n",
      "        \"end_ts_format\": null,\r\n",
      "        \"data_schema\": [\r\n",
      "            {\r\n",
      "                \"eye_color\": \"categorical\",\r\n",
      "                \"dob\": [\r\n",
      "                    \"dob\",\r\n",
      "                    [\r\n",
      "                        \"timestamp\",\r\n",
      "                        \"%m/%d/%Y\"\r\n",
      "                    ]\r\n",
      "                ]\r\n",
      "            }\r\n",
      "        ],\r\n",
      "        \"start_data_schema\": null,\r\n",
      "        \"end_data_schema\": null,\r\n",
      "        \"must_have\": []\r\n",
      "    },\r\n",
      "    \"dynamic\": [\r\n",
      "        {\r\n",
      "            \"input_df\": \"./sample_data/raw//admit_vitals.csv\",\r\n",
      "            \"type\": \"range\",\r\n",
      "            \"event_type\": [\r\n",
      "                \"OUTPATIENT_VISIT\",\r\n",
      "                \"ADMISSION\",\r\n",
      "                \"DISCHARGE\"\r\n",
      "            ],\r\n",
      "            \"subject_id_col\": \"MRN\",\r\n",
      "            \"ts_col\": null,\r\n",
      "            \"start_ts_col\": \"admit_date\",\r\n",
      "            \"end_ts_col\": \"disch_date\",\r\n",
      "            \"ts_format\": null,\r\n",
      "            \"start_ts_format\": \"%m/%d/%Y, %H:%M:%S\",\r\n",
      "            \"end_ts_format\": \"%m/%d/%Y, %H:%M:%S\",\r\n",
      "            \"data_schema\": [\r\n",
      "                {\r\n",
      "                    \"department\": \"categorical\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"start_data_schema\": [\r\n",
      "                {\r\n",
      "                    \"department\": \"categorical\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"end_data_schema\": [\r\n",
      "                {\r\n",
      "                    \"department\": \"categorical\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"must_have\": []\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"input_df\": \"./sample_data/raw//admit_vitals.csv\",\r\n",
      "            \"type\": \"event\",\r\n",
      "            \"event_type\": \"VITAL\",\r\n",
      "            \"subject_id_col\": \"MRN\",\r\n",
      "            \"ts_col\": \"vitals_date\",\r\n",
      "            \"start_ts_col\": null,\r\n",
      "            \"end_ts_col\": null,\r\n",
      "            \"ts_format\": \"%m/%d/%Y, %H:%M:%S\",\r\n",
      "            \"start_ts_format\": null,\r\n",
      "            \"end_ts_format\": null,\r\n",
      "            \"data_schema\": [\r\n",
      "                {\r\n",
      "                    \"HR\": \"float\",\r\n",
      "                    \"temp\": \"float\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"start_data_schema\": null,\r\n",
      "            \"end_data_schema\": null,\r\n",
      "            \"must_have\": []\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"input_df\": \"./sample_data/raw//labs.csv\",\r\n",
      "            \"type\": \"event\",\r\n",
      "            \"event_type\": \"LAB\",\r\n",
      "            \"subject_id_col\": \"MRN\",\r\n",
      "            \"ts_col\": \"timestamp\",\r\n",
      "            \"start_ts_col\": null,\r\n",
      "            \"end_ts_col\": null,\r\n",
      "            \"ts_format\": \"%H:%M:%S-%Y-%m-%d\",\r\n",
      "            \"start_ts_format\": null,\r\n",
      "            \"end_ts_format\": null,\r\n",
      "            \"data_schema\": [\r\n",
      "                {\r\n",
      "                    \"lab_name\": \"categorical\",\r\n",
      "                    \"lab_value\": \"float\"\r\n",
      "                }\r\n",
      "            ],\r\n",
      "            \"start_data_schema\": null,\r\n",
      "            \"end_data_schema\": null,\r\n",
      "            \"must_have\": []\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat processed/sample/input_schema.json | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ddf01",
   "metadata": {},
   "source": [
    "This object, stored in JSON format, is an instance of the `EventStream.data.config.DatasetSchema` object; interested readers can read more about it's specific formatting requirements there. We can see that this contains much of the same information as was in the initial `dataset.yaml` config shown above, now with some additional data added as well, such as recognizing that the `\"lab_name\"` column should be read in as a categorical type and `\"lab_value\"` as a float type.\n",
    "\n",
    "Beyond the input data schema, the model also writes out the ESGPT's input overall config object to disk, which stores information about which measurements the pipeline is instructed to extract. That object is stored in `config.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad61c29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"measurement_configs\": {\r\n",
      "        \"eye_color\": {\r\n",
      "            \"name\": \"eye_color\",\r\n",
      "            \"temporality\": \"static\",\r\n",
      "            \"modality\": \"single_label_classification\",\r\n",
      "            \"observation_frequency\": null,\r\n",
      "            \"functor\": null,\r\n",
      "            \"vocabulary\": null,\r\n",
      "            \"values_column\": null,\r\n",
      "            \"_measurement_metadata\": null\r\n",
      "        },\r\n",
      "        \"department\": {\r\n",
      "            \"name\": \"department\",\r\n",
      "            \"temporality\": \"dynamic\",\r\n",
      "            \"modality\": \"multi_label_classification\",\r\n",
      "            \"observation_frequency\": null,\r\n",
      "            \"functor\": null,\r\n",
      "            \"vocabulary\": null,\r\n",
      "            \"values_column\": null,\r\n",
      "            \"_measurement_metadata\": null\r\n",
      "        },\r\n",
      "        \"HR\": {\r\n",
      "            \"name\": \"HR\",\r\n",
      "            \"temporality\": \"dynamic\",\r\n",
      "            \"modality\": \"univariate_regression\",\r\n",
      "            \"observation_frequency\": null,\r\n",
      "            \"functor\": null,\r\n",
      "            \"vocabulary\": null,\r\n",
      "            \"values_column\": null,\r\n",
      "            \"_measurement_metadata\": null\r\n",
      "        },\r\n",
      "        \"temp\": {\r\n",
      "            \"name\": \"temp\",\r\n",
      "            \"temporality\": \"dynamic\",\r\n",
      "            \"modality\": \"univariate_regression\",\r\n",
      "            \"observation_frequency\": null,\r\n",
      "            \"functor\": null,\r\n",
      "            \"vocabulary\": null,\r\n",
      "            \"values_column\": null,\r\n",
      "            \"_measurement_metadata\": null\r\n",
      "        },\r\n",
      "        \"lab_name\": {\r\n",
      "            \"name\": \"lab_name\",\r\n",
      "            \"temporality\": \"dynamic\",\r\n",
      "            \"modality\": \"multivariate_regression\",\r\n",
      "            \"observation_frequency\": null,\r\n",
      "            \"functor\": null,\r\n",
      "            \"vocabulary\": null,\r\n",
      "            \"values_column\": \"lab_value\",\r\n",
      "            \"_measurement_metadata\": null\r\n",
      "        },\r\n",
      "        \"age\": {\r\n",
      "            \"name\": \"age\",\r\n",
      "            \"temporality\": \"functional_time_dependent\",\r\n",
      "            \"modality\": \"univariate_regression\",\r\n",
      "            \"observation_frequency\": null,\r\n",
      "            \"functor\": {\r\n",
      "                \"class\": \"AgeFunctor\",\r\n",
      "                \"params\": {\r\n",
      "                    \"dob_col\": \"dob\"\r\n",
      "                }\r\n",
      "            },\r\n",
      "            \"vocabulary\": null,\r\n",
      "            \"values_column\": null,\r\n",
      "            \"_measurement_metadata\": null\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"min_events_per_subject\": 3,\r\n",
      "    \"agg_by_time_scale\": \"1h\",\r\n",
      "    \"min_valid_column_observations\": 5,\r\n",
      "    \"min_valid_vocab_element_observations\": 5,\r\n",
      "    \"min_true_float_frequency\": 0.1,\r\n",
      "    \"min_unique_numerical_observations\": 20,\r\n",
      "    \"outlier_detector_config\": {\r\n",
      "        \"cls\": \"stddev_cutoff\",\r\n",
      "        \"stddev_cutoff\": 1.5\r\n",
      "    },\r\n",
      "    \"normalizer_config\": {\r\n",
      "        \"cls\": \"standard_scaler\"\r\n",
      "    },\r\n",
      "    \"save_dir\": \"/home/mmd/Projects/EventStreamGPT/sample_data/processed/sample\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat processed/sample/config.json | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49cfdd",
   "metadata": {},
   "source": [
    "Again, much of this information is simply a more verbose re-arrangement of the data specified in `dataset.yaml`. Notably, no information about the measurements has yet been filled in from the data, though it will eventually be added.\n",
    "\n",
    "```{note}\n",
    "This config structure illustrates a capability of the pipeline outside of the traditional input script format; namely, if one constructs the full config manually, one can pre-specify various measurement specific values (such as vocabulary, normalization parameters, etc.) to be used over what would be inferred from the data.\n",
    "```\n",
    "\n",
    "There is also the full, expanded hydra config stored in `hydra_config.yaml`, which can help aid in reproducibility.\n",
    "\n",
    "```{note}\n",
    "The final input to the constructor of the `EventStream.data.dataset_polars.Dataset` class can be seen in the documentation for its base class, `EventStream.data.dataset_base.DatasetBase` and takeas input:\n",
    "  1. A `config` object (like that shown in JSON form above.\n",
    "  2. Either the `subjects_df`, `events_df`, and `dynamic_measurements_df` dataframes directly or an `input_schema` `EventStream.data.config.DatasetSchema` object which is shown in `input_schema.json` above, which is used to construct the three dataframes from source. Currently, the immediate extraction output is not written to disk at all, so we can't directly inspect the `subjects_df`, `events_df`, and `dynamic_measurements_df` that result from our `input_schema`, but we can see their relative structure from the final, pre-processed dataframes which are written to disk, which we'll explore next.\n",
    "```\n",
    "\n",
    "#### Step 2: Data reading and pre-processing\n",
    "After normalizing the input configs, the pipeline next extracts the data from source and performs pre-processing on these dataframes. This pre-processing encompasses several steps, including:\n",
    "  1. Minimizing data types to minimizing memory/disk footprint.\n",
    "  2. Splitting data into train, hyperparameter tuning, and held out test sets.\n",
    "  3. Identifying categorical variable vocabularies.\n",
    "  4. Converting appropriate numerical variables to categorical.\n",
    "  5. Fitting numerical outlier detectors and normalizers.\n",
    "  6. Normalizing numerical data, removing outliers and infrequent vocabulary elements, and writing out processed `subjects_df`, `events_df`, and `dynamic_measurements_df` parquet files.\n",
    "\n",
    "##### Pre-processed Data Frames\n",
    "After this process is complete, we gain the following three files:\n",
    "\n",
    "```{note}\n",
    "We'll inspect the files manually here, but you can also load the dataset object and inspect them that way, which we'll do below.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3cf559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mmd mmd  12M Jul 17 11:41 processed/sample/dynamic_measurements_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 7.1M Jul 17 11:41 processed/sample/events_df.parquet\r\n",
      "-rw-rw-r-- 1 mmd mmd 2.7K Jul 17 11:41 processed/sample/subjects_df.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls --color -lah processed/sample/*_df.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbdd6b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subject_id</th><th>MRN</th><th>eye_color</th><th>dob</th></tr><tr><td>u8</td><td>cat</td><td>cat</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>0</td><td>&quot;310243&quot;</td><td>&quot;GREEN&quot;</td><td>1981-07-28 00:00:00</td></tr><tr><td>1</td><td>&quot;384198&quot;</td><td>&quot;BROWN&quot;</td><td>1985-04-15 00:00:00</td></tr><tr><td>2</td><td>&quot;520533&quot;</td><td>&quot;BROWN&quot;</td><td>1979-04-15 00:00:00</td></tr><tr><td>3</td><td>&quot;850710&quot;</td><td>&quot;HAZEL&quot;</td><td>1970-08-08 00:00:00</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 4)\n",
       "┌────────────┬────────┬───────────┬─────────────────────┐\n",
       "│ subject_id ┆ MRN    ┆ eye_color ┆ dob                 │\n",
       "│ ---        ┆ ---    ┆ ---       ┆ ---                 │\n",
       "│ u8         ┆ cat    ┆ cat       ┆ datetime[μs]        │\n",
       "╞════════════╪════════╪═══════════╪═════════════════════╡\n",
       "│ 0          ┆ 310243 ┆ GREEN     ┆ 1981-07-28 00:00:00 │\n",
       "│ 1          ┆ 384198 ┆ BROWN     ┆ 1985-04-15 00:00:00 │\n",
       "│ 2          ┆ 520533 ┆ BROWN     ┆ 1979-04-15 00:00:00 │\n",
       "│ 3          ┆ 850710 ┆ HAZEL     ┆ 1970-08-08 00:00:00 │\n",
       "└────────────┴────────┴───────────┴─────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We use polars to look at these parquet files:\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_cols(7)\n",
    "\n",
    "display(pl.scan_parquet('processed/sample/subjects_df.parquet').head(4).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bb7e9",
   "metadata": {},
   "source": [
    "The subjects dataframe `subjects_df` contains subject IDs (which have been re-named and normalized to occupy the minimal possible `uint` type (here `uint8`), and contains a categorical `eye_color` column for our static measurement, but `height` has been dropped as it wasn't included in our config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abe57f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>subject_id</th><th>timestamp</th><th>event_type</th><th>age</th><th>age_is_inlier</th></tr><tr><td>u32</td><td>u8</td><td>datetime[μs]</td><td>cat</td><td>f64</td><td>bool</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>2010-06-24 13:23:00</td><td>&quot;ADMISSION&amp;VITA…</td><td>-0.558276</td><td>true</td></tr><tr><td>1</td><td>0</td><td>2010-06-24 14:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.55825</td><td>true</td></tr><tr><td>2</td><td>0</td><td>2010-06-24 15:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558224</td><td>true</td></tr><tr><td>3</td><td>0</td><td>2010-06-24 16:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558199</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 6)\n",
       "┌──────────┬────────────┬─────────────────────┬─────────────────────┬───────────┬───────────────┐\n",
       "│ event_id ┆ subject_id ┆ timestamp           ┆ event_type          ┆ age       ┆ age_is_inlier │\n",
       "│ ---      ┆ ---        ┆ ---                 ┆ ---                 ┆ ---       ┆ ---           │\n",
       "│ u32      ┆ u8         ┆ datetime[μs]        ┆ cat                 ┆ f64       ┆ bool          │\n",
       "╞══════════╪════════════╪═════════════════════╪═════════════════════╪═══════════╪═══════════════╡\n",
       "│ 0        ┆ 0          ┆ 2010-06-24 13:23:00 ┆ ADMISSION&VITAL&LAB ┆ -0.558276 ┆ true          │\n",
       "│ 1        ┆ 0          ┆ 2010-06-24 14:23:00 ┆ VITAL&LAB           ┆ -0.55825  ┆ true          │\n",
       "│ 2        ┆ 0          ┆ 2010-06-24 15:23:00 ┆ VITAL&LAB           ┆ -0.558224 ┆ true          │\n",
       "│ 3        ┆ 0          ┆ 2010-06-24 16:23:00 ┆ VITAL&LAB           ┆ -0.558199 ┆ true          │\n",
       "└──────────┴────────────┴─────────────────────┴─────────────────────┴───────────┴───────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pl.scan_parquet('processed/sample/events_df.parquet').head(4).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105dd25",
   "metadata": {},
   "source": [
    "The events dataframe `events_df` contains event IDs, subject IDs, timestamps, event types, and our only functional time dependent measurement, `age`, in normalized form, alongside an inlier/outlier indicator column. We can also see several other properties:\n",
    "  1. That these data are sorted, first by `subject_id` then by `event_id` (equivalently, by `timestamp`).\n",
    "  2. That event timestamps are separated by precisely 1 hour, which was our input aggregation window.\n",
    "  3. That event types have been aggregated into merged categories during aggregation. E.g., event 1 with event type `VITAL&LAB` reflects that events of type `VITAL` and `LAB` have been merged together. This is to ensure that no subject has two distinct events at the same timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2afdd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Measurement Columns:\n",
      "  * measurement_id\n",
      "  * department\n",
      "  * HR\n",
      "  * temp\n",
      "  * lab_name\n",
      "  * lab_value\n",
      "  * event_id\n",
      "  * HR_is_inlier\n",
      "  * temp_is_inlier\n",
      "  * lab_name_is_inlier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>measurement_id</th><th>department</th><th>HR</th><th>&hellip;</th><th>HR_is_inlier</th><th>temp_is_inlier</th><th>lab_name_is_inlier</th></tr><tr><td>u32</td><td>cat</td><td>f64</td><td>&hellip;</td><td>bool</td><td>bool</td><td>bool</td></tr></thead><tbody><tr><td>0</td><td>&quot;CARDIAC&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 10)\n",
       "┌────────────────┬────────────┬──────┬──────┬───┬──────────────┬────────────────┬──────────────────┐\n",
       "│ measurement_id ┆ department ┆ HR   ┆ temp ┆ … ┆ HR_is_inlier ┆ temp_is_inlier ┆ lab_name_is_inli │\n",
       "│ ---            ┆ ---        ┆ ---  ┆ ---  ┆   ┆ ---          ┆ ---            ┆ er               │\n",
       "│ u32            ┆ cat        ┆ f64  ┆ f64  ┆   ┆ bool         ┆ bool           ┆ ---              │\n",
       "│                ┆            ┆      ┆      ┆   ┆              ┆                ┆ bool             │\n",
       "╞════════════════╪════════════╪══════╪══════╪═══╪══════════════╪════════════════╪══════════════════╡\n",
       "│ 0              ┆ CARDIAC    ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "│ 1              ┆ PULMONARY  ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "│ 2              ┆ PULMONARY  ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "│ 3              ┆ PULMONARY  ┆ null ┆ null ┆ … ┆ null         ┆ null           ┆ null             │\n",
       "└────────────────┴────────────┴──────┴──────┴───┴──────────────┴────────────────┴──────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pl.scan_parquet('processed/sample/dynamic_measurements_df.parquet')\n",
    "print(\"Dynamic Measurement Columns:\\n  * \" + '\\n  * '.join(df.columns))\n",
    "display(df.head(4).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829516ab",
   "metadata": {},
   "source": [
    "The dynamic measurements dataframe `dynamic_measurements_df` has an ID column, an `event_id` linking column, and then all our measurements, recorded with missingness.\n",
    "\n",
    "##### Fit Measurement Properties\n",
    "In all of these dataframes, we can see the outputs from our learned vocabularies, outlier detector models, and normalizer models. How can we determine what fit parameters were used to make those distinctions? These data are stored in the `inferred_measurement_metadata` objects. The overall container is stored in `inferred_measurement_configs.json`, which contains an object linking measurement names to overall configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e3fa36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"eye_color\": {\r\n",
      "        \"name\": \"eye_color\",\r\n",
      "        \"temporality\": \"static\",\r\n",
      "        \"modality\": \"single_label_classification\",\r\n",
      "        \"observation_frequency\": 1.0,\r\n",
      "        \"functor\": null,\r\n",
      "        \"vocabulary\": {\r\n",
      "            \"vocabulary\": [\r\n",
      "                \"UNK\",\r\n",
      "                \"BROWN\",\r\n",
      "                \"BLUE\",\r\n",
      "                \"HAZEL\",\r\n",
      "                \"GREEN\"\r\n",
      "            ],\r\n",
      "            \"obs_frequencies\": [\r\n",
      "                0.0,\r\n",
      "                0.5125,\r\n",
      "                0.2125,\r\n",
      "                0.175,\r\n",
      "                0.1\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        \"values_column\": null,\r\n",
      "        \"_measurement_metadata\": null\r\n",
      "    },\r\n",
      "    \"department\": {\r\n",
      "        \"name\": \"department\",\r\n",
      "        \"temporality\": \"dynamic\",\r\n",
      "        \"modality\": \"multi_label_classification\",\r\n",
      "        \"observation_frequency\": 0.0007383747656851046,\r\n",
      "        \"functor\": null,\r\n",
      "        \"vocabulary\": {\r\n",
      "            \"vocabulary\": [\r\n",
      "                \"UNK\",\r\n",
      "                \"CARDIAC\",\r\n",
      "                \"PULMONARY\",\r\n",
      "                \"ORTHOPEDIC\"\r\n",
      "            ],\r\n",
      "            \"obs_frequencies\": [\r\n",
      "                0.0,\r\n",
      "                0.3870967741935484,\r\n",
      "                0.36129032258064514,\r\n",
      "                0.25161290322580643\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        \"values_column\": null,\r\n",
      "        \"_measurement_metadata\": null\r\n",
      "    },\r\n",
      "    \"HR\": {\r\n",
      "        \"name\": \"HR\",\r\n",
      "        \"temporality\": \"dynamic\",\r\n",
      "        \"modality\": \"univariate_regression\",\r\n",
      "        \"observation_frequency\": 0.04316872339766721,\r\n",
      "        \"functor\": null,\r\n",
      "        \"vocabulary\": null,\r\n",
      "        \"values_column\": null,\r\n",
      "        \"_measurement_metadata\": \"/home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/inferred_measurement_metadata/HR.csv\"\r\n",
      "    },\r\n",
      "    \"temp\": {\r\n",
      "        \"name\": \"temp\",\r\n",
      "        \"temporality\": \"dynamic\",\r\n",
      "        \"modality\": \"univariate_regression\",\r\n",
      "        \"observation_frequency\": 0.04316872339766721,\r\n",
      "        \"functor\": null,\r\n",
      "        \"vocabulary\": null,\r\n",
      "        \"values_column\": null,\r\n",
      "        \"_measurement_metadata\": \"/home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/inferred_measurement_metadata/temp.csv\"\r\n",
      "    },\r\n",
      "    \"lab_name\": {\r\n",
      "        \"name\": \"lab_name\",\r\n",
      "        \"temporality\": \"dynamic\",\r\n",
      "        \"modality\": \"multivariate_regression\",\r\n",
      "        \"observation_frequency\": 0.9957602997325178,\r\n",
      "        \"functor\": null,\r\n",
      "        \"vocabulary\": {\r\n",
      "            \"vocabulary\": [\r\n",
      "                \"UNK\",\r\n",
      "                \"SpO2\",\r\n",
      "                \"creatinine\",\r\n",
      "                \"potassium\",\r\n",
      "                \"SOFA__EQ_1\",\r\n",
      "                \"SOFA__EQ_2\",\r\n",
      "                \"GCS__EQ_1\",\r\n",
      "                \"SOFA__EQ_3\",\r\n",
      "                \"GCS__EQ_4\",\r\n",
      "                \"GCS__EQ_3\",\r\n",
      "                \"GCS__EQ_2\",\r\n",
      "                \"SOFA__EQ_4\",\r\n",
      "                \"GCS__EQ_5\",\r\n",
      "                \"GCS__EQ_6\",\r\n",
      "                \"GCS__EQ_7\",\r\n",
      "                \"GCS__EQ_8\",\r\n",
      "                \"GCS__EQ_9\",\r\n",
      "                \"GCS__EQ_10\",\r\n",
      "                \"GCS__EQ_11\",\r\n",
      "                \"GCS__EQ_15\",\r\n",
      "                \"GCS__EQ_12\",\r\n",
      "                \"GCS__EQ_13\",\r\n",
      "                \"GCS__EQ_14\"\r\n",
      "            ],\r\n",
      "            \"obs_frequencies\": [\r\n",
      "                0.0,\r\n",
      "                0.8358588793794315,\r\n",
      "                0.03923998528282067,\r\n",
      "                0.037467631381850504,\r\n",
      "                0.029761212891340198,\r\n",
      "                0.012728087876592431,\r\n",
      "                0.01162429942643202,\r\n",
      "                0.0051816007292170094,\r\n",
      "                0.0037605714180841275,\r\n",
      "                0.003734353165111196,\r\n",
      "                0.003473918518913411,\r\n",
      "                0.0026139598214012607,\r\n",
      "                0.0025877415684283293,\r\n",
      "                0.002348281524608889,\r\n",
      "                0.0019506380211860963,\r\n",
      "                0.0015503726924660103,\r\n",
      "                0.0013528618534032603,\r\n",
      "                0.0011116539260522915,\r\n",
      "                0.0009683274764669331,\r\n",
      "                0.000848597454557213,\r\n",
      "                0.0007778081715302983,\r\n",
      "                0.0006266162460530605,\r\n",
      "                0.00043260117405336814\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        \"values_column\": \"lab_value\",\r\n",
      "        \"_measurement_metadata\": \"/home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/inferred_measurement_metadata/lab_name.csv\"\r\n",
      "    },\r\n",
      "    \"age\": {\r\n",
      "        \"name\": \"age\",\r\n",
      "        \"temporality\": \"functional_time_dependent\",\r\n",
      "        \"modality\": \"univariate_regression\",\r\n",
      "        \"observation_frequency\": 1.0,\r\n",
      "        \"functor\": {\r\n",
      "            \"class\": \"AgeFunctor\",\r\n",
      "            \"params\": {\r\n",
      "                \"dob_col\": \"dob\"\r\n",
      "            }\r\n",
      "        },\r\n",
      "        \"vocabulary\": null,\r\n",
      "        \"values_column\": null,\r\n",
      "        \"_measurement_metadata\": \"/home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/inferred_measurement_metadata/age.csv\"\r\n",
      "    }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat processed/sample/inferred_measurement_configs.json | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5d5dd",
   "metadata": {},
   "source": [
    "We can see that these objects contain the full vocabularies learned, as well as (for numerical measurements) internal links to further measurement metadata `csv` files. These `csv` files contain more detailed statistics for numerical data, such as outlier detector and normalizer models. Let's inspect two of these, one for the `multivariate_regression` measurement `lab_name` and one for the `univariate_regression` measurement `age`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a7fc54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| lab_name   | value_type          | outlier_model                                                               | normalizer                                                |\r\n",
      "| ---------- | ------------------- | --------------------------------------------------------------------------- | --------------------------------------------------------- |\r\n",
      "| creatinine | float               | {'thresh_large_': 15940.942090515884, 'thresh_small_': -15715.577631719556} | {'mean_': 1.3238022046456384, 'std_': 1.0517759234408144} |\r\n",
      "| SOFA       | categorical_integer | {'thresh_large_': None, 'thresh_small_': None}                              | {'mean_': None, 'std_': None}                             |\r\n",
      "| potassium  | float               | {'thresh_large_': 12622.159050165134, 'thresh_small_': -12472.58665045867}  | {'mean_': 4.810794749300743, 'std_': 1.6868907048792603}  |\r\n",
      "| GCS        | categorical_integer | {'thresh_large_': None, 'thresh_small_': None}                              | {'mean_': None, 'std_': None}                             |\r\n",
      "| SpO2       | integer             | {'thresh_large_': 15413.86526239094, 'thresh_small_': -15105.062867218923}  | {'mean_': 50.895888949886285, 'std_': 2.4655926502330923} |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" processed/sample/inferred_measurement_metadata/lab_name.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8afb9bf",
   "metadata": {},
   "source": [
    "We can see that the `lab_name.csv` file contains a dataframe mapping the `lab_name` (the categorical component of this multivariate regression task) to the inferred `value_type` (whether the value is a `float`, `integer`, `categorical_float`, or `categorical_integer`), `outlier_model` parameters, and `normalizer` parameters. From this, we can see that the `GCS` lab test has been interpreted as a `categorical_integer`, and from the `vocabulary` in the prior JSON object, we can see that it takes on values ranging from 1 to 15. In contrast, we can see that the `SpO2` lab value is a float a mean of 50.9 (which, to be clear, is a _bad_ real-world SpO2), and has an inferred outlier threshold of approximately $\\pm15000$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bbe8bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mmd/Programs/miniconda3/envs/RAMMS_cpu/lib/python3.10/site-packages/agate/utils.py:274: UnnamedColumnWarning: Column 0 has no name. Using \"a\".\r\n",
      "| a             | age                                                                      |\r\n",
      "| ------------- | ------------------------------------------------------------------------ |\r\n",
      "| value_type    | float                                                                    |\r\n",
      "| outlier_model | {'thresh_large_': 39.4032082341397, 'thresh_small_': 22.911637579463825} |\r\n",
      "| normalizer    | {'mean_': 31.398075972111368, 'std_': 4.460737015690829}                 |\r\n"
     ]
    }
   ],
   "source": [
    "!csvlook -d \",\" --max-column-width=100 processed/sample/inferred_measurement_metadata/age.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f031a6",
   "metadata": {},
   "source": [
    "In contrast to the `multivariate_regression` measurement file, the `univariate` `age.csv` file contains a series representation mapping the three non-categorical-index columns of the prior file to their unique value for `age` alone. We can see that `age` is a floating point value, with a mean of $31.4\\pm 4.5$ within the \"inlier\" range of $22.9 - 39.4$.\n",
    "\n",
    "##### Inspecting the dataset object.\n",
    "We can also look at the same content through the actual object oriented dataset interface, which contains all the above information and more, as loaded through some of the other files in this directory, such as `E.pkl` which contains other dataset attributes. Let's do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd39847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a247480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "\n",
    "pl.Config.set_tbl_cols(7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a9eb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"processed/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8addfc",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f11a578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9813e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/subjects_df.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subject_id</th><th>MRN</th><th>eye_color</th><th>dob</th></tr><tr><td>u8</td><td>cat</td><td>cat</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>0</td><td>&quot;310243&quot;</td><td>&quot;GREEN&quot;</td><td>1981-07-28 00:00:00</td></tr><tr><td>1</td><td>&quot;384198&quot;</td><td>&quot;BROWN&quot;</td><td>1985-04-15 00:00:00</td></tr><tr><td>2</td><td>&quot;520533&quot;</td><td>&quot;BROWN&quot;</td><td>1979-04-15 00:00:00</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌────────────┬────────┬───────────┬─────────────────────┐\n",
       "│ subject_id ┆ MRN    ┆ eye_color ┆ dob                 │\n",
       "│ ---        ┆ ---    ┆ ---       ┆ ---                 │\n",
       "│ u8         ┆ cat    ┆ cat       ┆ datetime[μs]        │\n",
       "╞════════════╪════════╪═══════════╪═════════════════════╡\n",
       "│ 0          ┆ 310243 ┆ GREEN     ┆ 1981-07-28 00:00:00 │\n",
       "│ 1          ┆ 384198 ┆ BROWN     ┆ 1985-04-15 00:00:00 │\n",
       "│ 2          ┆ 520533 ┆ BROWN     ┆ 1979-04-15 00:00:00 │\n",
       "└────────────┴────────┴───────────┴─────────────────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESD.subjects_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fee31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading events from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/events_df.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>subject_id</th><th>timestamp</th><th>event_type</th><th>age</th><th>age_is_inlier</th></tr><tr><td>u32</td><td>u8</td><td>datetime[μs]</td><td>cat</td><td>f64</td><td>bool</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>2010-06-24 13:23:00</td><td>&quot;ADMISSION&amp;VITA…</td><td>-0.558276</td><td>true</td></tr><tr><td>1</td><td>0</td><td>2010-06-24 14:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.55825</td><td>true</td></tr><tr><td>2</td><td>0</td><td>2010-06-24 15:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558224</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 6)\n",
       "┌──────────┬────────────┬─────────────────────┬─────────────────────┬───────────┬───────────────┐\n",
       "│ event_id ┆ subject_id ┆ timestamp           ┆ event_type          ┆ age       ┆ age_is_inlier │\n",
       "│ ---      ┆ ---        ┆ ---                 ┆ ---                 ┆ ---       ┆ ---           │\n",
       "│ u32      ┆ u8         ┆ datetime[μs]        ┆ cat                 ┆ f64       ┆ bool          │\n",
       "╞══════════╪════════════╪═════════════════════╪═════════════════════╪═══════════╪═══════════════╡\n",
       "│ 0        ┆ 0          ┆ 2010-06-24 13:23:00 ┆ ADMISSION&VITAL&LAB ┆ -0.558276 ┆ true          │\n",
       "│ 1        ┆ 0          ┆ 2010-06-24 14:23:00 ┆ VITAL&LAB           ┆ -0.55825  ┆ true          │\n",
       "│ 2        ┆ 0          ┆ 2010-06-24 15:23:00 ┆ VITAL&LAB           ┆ -0.558224 ┆ true          │\n",
       "└──────────┴────────────┴─────────────────────┴─────────────────────┴───────────┴───────────────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESD.events_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36c472e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dynamic_measurements from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/dynamic_measurements_df.parquet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>measurement_id</th><th>department</th><th>HR</th><th>&hellip;</th><th>HR_is_inlier</th><th>temp_is_inlier</th><th>lab_name_is_inlier</th></tr><tr><td>u32</td><td>cat</td><td>f64</td><td>&hellip;</td><td>bool</td><td>bool</td><td>bool</td></tr></thead><tbody><tr><td>189</td><td>&quot;PULMONARY&quot;</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1980</td><td>null</td><td>-0.865191</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>2112</td><td>null</td><td>-1.006687</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>23396</td><td>null</td><td>-0.840872</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>36963</td><td>null</td><td>-1.128285</td><td>&hellip;</td><td>true</td><td>true</td><td>null</td></tr><tr><td>83676</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>107279</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>315525</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>1076810</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr><tr><td>1248235</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 10)\n",
       "┌────────────┬────────────┬───────────┬──────────┬───┬───────────────┬──────────────┬──────────────┐\n",
       "│ measuremen ┆ department ┆ HR        ┆ temp     ┆ … ┆ HR_is_inlier  ┆ temp_is_inli ┆ lab_name_is_ │\n",
       "│ t_id       ┆ ---        ┆ ---       ┆ ---      ┆   ┆ ---           ┆ er           ┆ inlier       │\n",
       "│ ---        ┆ cat        ┆ f64       ┆ f64      ┆   ┆ bool          ┆ ---          ┆ ---          │\n",
       "│ u32        ┆            ┆           ┆          ┆   ┆               ┆ bool         ┆ bool         │\n",
       "╞════════════╪════════════╪═══════════╪══════════╪═══╪═══════════════╪══════════════╪══════════════╡\n",
       "│ 189        ┆ PULMONARY  ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ null         │\n",
       "│ 1980       ┆ null       ┆ -0.865191 ┆ 1.246792 ┆ … ┆ true          ┆ true         ┆ null         │\n",
       "│ 2112       ┆ null       ┆ -1.006687 ┆ 0.926212 ┆ … ┆ true          ┆ true         ┆ null         │\n",
       "│ 23396      ┆ null       ┆ -0.840872 ┆ 1.086504 ┆ … ┆ true          ┆ true         ┆ null         │\n",
       "│ …          ┆ …          ┆ …         ┆ …        ┆ … ┆ …             ┆ …            ┆ …            │\n",
       "│ 107279     ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "│ 315525     ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "│ 1076810    ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "│ 1248235    ┆ null       ┆ null      ┆ null     ┆ … ┆ null          ┆ null         ┆ true         │\n",
       "└────────────┴────────────┴───────────┴──────────┴───┴───────────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESD.dynamic_measurements_df.filter(pl.col(\"event_id\") == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac71cca",
   "metadata": {},
   "source": [
    "### Other Files\n",
    "#### `.logs`\n",
    "\n",
    "This sub-directory contains a hydra run log file for this dataset build run. As the pipeline currently doesn't\n",
    "take advantage of the python logging module at all, the files it contains are empty.\n",
    "\n",
    "##### `E.pkl`\n",
    "\n",
    "This file contains the class object itself and a collection of its attributes. It, importantly, _does not_\n",
    "contain the nested dataframes (`subjects_df`, `events_df`, `dynamic_measurements_df`), as these are stored in\n",
    "the files mentioned above and loaded lazily by the object during use, and similarly does not store the learned\n",
    "measurement metadata files also mentioned above, which are lazily loaded in the same way.\n",
    "\n",
    "```{warning}\n",
    "Currently, this lazy saving/loading uses absolute paths, which makes transferring datasets to new locations more challenging! Paths can be modified in the class and config object locally to fix this problem, but it is a challenge.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
