{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abbd7c4",
   "metadata": {},
   "source": [
    "# Local Data Tutorial\n",
    "\n",
    "In this tutorial, rather than running real models and configurations over MIMIC-IV, we'll work with a set of\n",
    "local, synthetic files distributed with this repository, with the goal being to fully explore the details of\n",
    "this pipeline. This tutorial will consist of both content on this page, running certain scripts on one's local\n",
    "machine, and some jupyter notebooks. We will walk through the entire pipeline with these local examples and\n",
    "discuss limitations of the pipeline, details of classes, scripts, etc.\n",
    "\n",
    "## Synthetic Data\n",
    "\n",
    "For this tutorial, we'll use the three synthetic data files distributed in the [sample_data/raw](<>) folder in\n",
    "the repository:\n",
    "\n",
    "```bash\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/raw] [base] running_local_example(+6/-5)+* ± ls -lah\n",
    "total 53M\n",
    "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 12:42 .\n",
    "drwxrwxr-x 5 mmd mmd 4.0K Jul 14 16:39 ..\n",
    "-rw-rw-r-- 1 mmd mmd 3.6M Jul 14 16:31 admit_vitals.csv\n",
    "-rw-rw-r-- 1 mmd mmd  50M Jul 14 16:32 labs.csv\n",
    "-rw-rw-r-- 1 mmd mmd 4.2K Jul 14 16:31 subjects.csv\n",
    "```\n",
    "\n",
    "```{note}\n",
    "To see how those files are generated, look at [sample_data/generate_synthetic_data.ipynb]().\n",
    "```\n",
    "\n",
    "These files contain the following data:\n",
    "\n",
    "### `subjects.csv`\n",
    "\n",
    "This file contains per-subject data. It has a subject identifier, a date of birth, a categorical static\n",
    "measurement (eye color), and a continuous static measurement (height):\n",
    "\n",
    "```{literalinclude} ../../sample_data/raw/subjects.csv\n",
    "---\n",
    "lines: 1-3\n",
    "language: csv\n",
    "---\n",
    "```\n",
    "\n",
    "This file is arranged with one row per subject.\n",
    "\n",
    "### `admit_vitals.csv`\n",
    "\n",
    "This file contains dynamic data quantifying both fictional subject hospital admissions, and fictional vitals\n",
    "signs measured for those subjects.\n",
    "\n",
    "```{literalinclude} ../../sample_data/raw/admit_vitals.csv\n",
    "---\n",
    "lines: 1-3\n",
    "language: csv\n",
    "---\n",
    "```\n",
    "\n",
    "Each row of this file records a unique vitals sign measurement for a patient, affiliated with the associated\n",
    "admission listed in the row. This means that admission level information is _heavily duplicated_ within this\n",
    "file, which is a phenomena sometimes observed in real data, and something we'll need to account for in our\n",
    "pipeline's setup.\n",
    "\n",
    "### `labs.csv`\n",
    "\n",
    "This file contains dynamic data quantifying fictional subject laboratory test measurements.\n",
    "\n",
    "```{literalinclude} ../../sample_data/raw/labs.csv\n",
    "---\n",
    "lines: 1-3\n",
    "language: csv\n",
    "---\n",
    "```\n",
    "\n",
    "Each row of this file contains a record of a particular lab test measured for a subject.\n",
    "\n",
    "## Processing Synthetic Data with ESGPT\n",
    "\n",
    "Now that we see the form of this synthetic data, we can examine how to process it with Event Stream GPT. From\n",
    "the base directory of the ESGPT repository, we can run the following command:\n",
    "\n",
    "```bash\n",
    "PYTHONPATH=$(pwd):$PYTHONPATH ./scripts/build_dataset.py \\\n",
    "\t--config-path=\"$(pwd)/sample_data/\" \\\n",
    "\t--config-name=dataset \\\n",
    "\t\"hydra.searchpath=[$(pwd)/configs]\"\n",
    "```\n",
    "\n",
    "You should see as output the printed line `Empty new events dataframe of type OUTPATIENT_VISIT!`, but\n",
    "otherwise nothing. Before we proceed further, let's break down what this process has done, and how it could do\n",
    "things differently. Clearly, the critical input to this pipeline is the dataset configuration file. But,\n",
    "before we walk through this file, let's take a look at what the pipeline has produced.\n",
    "\n",
    "### Inspecting the Output\n",
    "\n",
    "The entire output dataset is stored in the [sample_data/processed/sample](<>) directory. Let's inspect its\n",
    "contents:\n",
    "\n",
    "```bash\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example(+6/-5)+* ± ls -lah\n",
    "total 19M\n",
    "drwxrwxr-x 5 mmd mmd 4.0K Jul 14 16:32 .\n",
    "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 16:32 ..\n",
    "-rw-rw-r-- 1 mmd mmd 1.8K Jul 14 16:32 config.json\n",
    "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 16:32 DL_reps\n",
    "-rw-rw-r-- 1 mmd mmd  12M Jul 14 16:32 dynamic_measurements_df.parquet\n",
    "-rw-rw-r-- 1 mmd mmd 5.2K Jul 14 16:32 E.pkl\n",
    "-rw-rw-r-- 1 mmd mmd 7.0M Jul 14 16:32 events_df.parquet\n",
    "-rw-rw-r-- 1 mmd mmd 1.5K Jul 14 16:32 hydra_config.yaml\n",
    "-rw-rw-r-- 1 mmd mmd 2.3K Jul 14 16:32 inferred_measurement_configs.json\n",
    "drwxrwxr-x 2 mmd mmd 4.0K Jul 14 16:32 inferred_measurement_metadata\n",
    "-rw-rw-r-- 1 mmd mmd 1.7K Jul 14 16:32 input_schema.json\n",
    "drwxrwxr-x 3 mmd mmd 4.0K Jul 14 16:32 .logs\n",
    "-rw-rw-r-- 1 mmd mmd 2.7K Jul 14 16:32 subjects_df.parquet\n",
    "-rw-rw-r-- 1 mmd mmd  771 Jul 14 16:32 vocabulary_config.json\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example(+6/-5)+* ± du -sh .\n",
    "30M     .\n",
    "```\n",
    "\n",
    "We can see that this directory contains a set of files and sub-directories, and that in total it takes up only\n",
    "30 MB of disk space. Note that this is in contrast to the original, raw data, which took 53 MB on disk.\n",
    "\n",
    "Each of these files contains different information about this synthetic dataset. Let's inspect them and see\n",
    "what they contain. We'll go in a rough order that corresponds with where these files fit into the broader\n",
    "pipeline.\n",
    "\n",
    "#### Input & Logging Files\n",
    "\n",
    "##### `hydra_config.yaml`\n",
    "\n",
    "This file contains the full, resolved hydra input config to the dataset script. Whereas [`dataset.yaml`](<>)\n",
    "(the input config file used in the script run above) relies on some default values in the built-in\n",
    "[`dataset_base.yaml`](<>) config, the `hydra_config.yaml` is fully self-sufficient and can be used to reproduce\n",
    "the pipeline run in its entirety. In this case, it is very similar to the [`dataset.yaml`](<>) file which we'll\n",
    "inspect in more detail later, so we won't include it here.\n",
    "\n",
    "##### `input_schema.json`\n",
    "\n",
    "This file contains a processed version of the input data frame schemas used to read the raw data from disk. It\n",
    "is produced from the input [`dataset.yaml`](<>) config file (which we'll discuss in more detail later), and is\n",
    "stored in JSON format. This can be helpful to validate exactly what sources were read in what way. It is not\n",
    "used in any downstream pipeline components, so is not essential to understand.\n",
    "\n",
    "```{literalinclude} ../../sample_data/processed/sample/input_schema.json\n",
    "---\n",
    "language: json\n",
    "---\n",
    "```\n",
    "\n",
    "##### `.logs`\n",
    "\n",
    "This sub-directory contains a hydra run log file for this dataset build run. As the pipeline currently doesn't\n",
    "take advantage of the python logging module at all, the files it contains are empty.\n",
    "\n",
    "```bash\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example(+6/-5)+* ± ls .logs/build_sample.log\n",
    ".logs/build_sample.log\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example(+6/-5)+* ± cat .logs/build_sample.log\n",
    "```\n",
    "\n",
    "#### Output Files\n",
    "\n",
    "##### Dataset Configuration & Learned Measurement Metadata\n",
    "\n",
    "###### `config.json`\n",
    "\n",
    "This is the dataset's input config file. It contains the input measurement specifications and control\n",
    "parameters for the dataset pipeline. This is largely set from the input `dataset.yaml` config.\n",
    "\n",
    "###### `inferred_measurement_configs.json` & `inferred_measurement_metadata`\n",
    "\n",
    "These represent the inferred pre-processing parameters for the inferred measurements. This is stored in two\n",
    "forms: First, most data about the inferred measurements is stored in a flat JSON file,\n",
    "`inferred_measurement_configs.json`. This file contains an object whose keys are measurement names and whose\n",
    "values are configuration objects describing the measurements.\n",
    "\n",
    "The full file looks like this:\n",
    "\n",
    "```{literalinclude} ../../sample_data/processed/sample/inferred_measurement_configs.json\n",
    "---\n",
    "language: json\n",
    "---\n",
    "```\n",
    "\n",
    "To isolate a single measurement, we can examine the configuration for `'eye_color'`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"eye_color\": {\n",
    "    \"name\": \"eye_color\",\n",
    "    \"temporality\": \"static\",\n",
    "    \"modality\": \"single_label_classification\",\n",
    "    \"observation_frequency\": 1.0,\n",
    "    \"functor\": null,\n",
    "    \"vocabulary\": {\n",
    "      \"vocabulary\": [\n",
    "        \"UNK\",\n",
    "        \"BROWN\",\n",
    "        \"BLUE\",\n",
    "        \"HAZEL\",\n",
    "        \"GREEN\"\n",
    "      ],\n",
    "      \"obs_frequencies\": [\n",
    "        0.0,\n",
    "        0.5125,\n",
    "        0.2125,\n",
    "        0.175,\n",
    "        0.1\n",
    "      ]\n",
    "    },\n",
    "    \"values_column\": null,\n",
    "    \"_measurement_metadata\": null\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We can see that this configuration object details several facts about the eye color measurement:\n",
    "\n",
    "- That it is a static, single-label classification measurement (these were specified in the input config)\n",
    "- That this is observed on 100% of subjects.\n",
    "- That the relative frequencies of the categories \"Brown\", \"Blue\", \"Hazel\", \"Green\" are 51.25%, 21.25%,\n",
    "  17.5%, and 10%, respectively.\n",
    "\n",
    "To see a different measurement, one that is a multivariate regression measurement, we can inspect the lab\n",
    "tests measurement configs:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"lab_name\": {\n",
    "    \"name\": \"lab_name\",\n",
    "    \"temporality\": \"dynamic\",\n",
    "    \"modality\": \"multivariate_regression\",\n",
    "    \"observation_frequency\": 0.9953452513588434,\n",
    "    \"functor\": null,\n",
    "    \"vocabulary\": {\n",
    "      \"vocabulary\": [\n",
    "        \"UNK\",\n",
    "        \"SpO2\",\n",
    "        \"creatinine\",\n",
    "        \"potassium\",\n",
    "        \"SOFA__EQ_1\",\n",
    "        \"GCS__EQ_1\",\n",
    "        \"SOFA__EQ_2\",\n",
    "        \"SOFA__EQ_3\",\n",
    "        \"GCS__EQ_4\",\n",
    "        \"GCS__EQ_3\",\n",
    "        \"GCS__EQ_2\",\n",
    "        \"GCS__EQ_5\",\n",
    "        \"GCS__EQ_6\",\n",
    "        \"GCS__EQ_7\",\n",
    "        \"SOFA__EQ_4\",\n",
    "        \"GCS__EQ_8\",\n",
    "        \"GCS__EQ_9\",\n",
    "        \"GCS__EQ_10\",\n",
    "        \"GCS__EQ_11\",\n",
    "        \"GCS__EQ_15\",\n",
    "        \"GCS__EQ_12\",\n",
    "        \"GCS__EQ_13\",\n",
    "        \"GCS__EQ_14\",\n",
    "        \"SOFA__EQ_1000000\",\n",
    "        \"GCS__EQ_1000000\"\n",
    "      ],\n",
    "      \"obs_frequencies\": [\n",
    "        0.0,\n",
    "        0.8259984895186395,\n",
    "        0.04326148962598335,\n",
    "        0.042245556731226326,\n",
    "        0.027447439849105214,\n",
    "        0.013256007422060696,\n",
    "        0.01155863274600911,\n",
    "        0.004522818236187147,\n",
    "        0.0045065249727806655,\n",
    "        0.004329215929827789,\n",
    "        0.003943928171627486,\n",
    "        0.0029251199950928526,\n",
    "        0.0027363098250295197,\n",
    "        0.0023232276763122785,\n",
    "        0.0022705141770560182,\n",
    "        0.0018171780834521783,\n",
    "        0.0015440263145788287,\n",
    "        0.001292918372667188,\n",
    "        0.0010964407845302172,\n",
    "        0.0009066721872076797,\n",
    "        0.000854917115210624,\n",
    "        0.0006613148088512674,\n",
    "        0.0004907147567128245,\n",
    "        5.750563555228412e-06,\n",
    "        4.792136296023677e-06\n",
    "      ]\n",
    "    },\n",
    "    \"values_column\": \"lab_value\",\n",
    "    \"_measurement_metadata\": \"/home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/inferred_measurement_metadata/lab_name.csv\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Here, in addition to the same information we see for eye color, we also see listed the associated values\n",
    "column for this multivariate regression, and also a path to a measurement metadata object that contains more\n",
    "statistics for this measurement. In addition, we can also see that under this configuration, the system has\n",
    "expanded the two laboratory tests `'SOFA'` and `'GCS'` into categorical options.\n",
    "\n",
    "We can inspect the detailed measurement metadata linked in this config object by looking at the csv file in\n",
    "question.\n",
    "\n",
    "```{literalinclude} ../../sample_data/processed/sample/inferred_measurement_metadata/lab_name.csv\n",
    "---\n",
    "language: csv\n",
    "---\n",
    "```\n",
    "\n",
    "Within this file, we see a dataframe containing information about the different laboratory tests that the\n",
    "system has processed, including their value type, information about the learned outlier model, and information\n",
    "learned about their normalization variables. For example, the system has inferred that the GCS and SOFA scores\n",
    "are categorical, integer variables, the SpO2 lab is an integer variable, and the potassium and creatinine labs\n",
    "are floating point labs. Further, it has learned outlier bounds for the various continuous laboratory tests\n",
    "and has fit the mean and standard deviation of the laboratory test values for these as well.\n",
    "\n",
    "##### Processed DataFrames\n",
    "\n",
    "###### `subjects_df.parquet`, `events_df.parquet`, `dynamic_measurements_df.parquet`\n",
    "\n",
    "These files are the output, processed, internally represented versions of the raw input data, organized\n",
    "according to the event-stream data model (see the Usage Guide for more information on that data model). \n",
    "\n",
    "###### `DL_reps`\n",
    "\n",
    "This directory contains the deep-learning formatted representtaions of the data. It is suitable for rapidly\n",
    "iterating through batches of subject time-series, but less well suited towards querying and data manipulation.\n",
    "\n",
    "```bash\n",
    "[mmd:~/Projects/EventStreamGPT/sample_data/processed/sample] [base] running_local_example+ ± ls DL_reps/\n",
    "held_out_0.parquet train_0.parquet tuning_0.parquet\n",
    "```\n",
    "\n",
    "##### Overall Class File\n",
    "\n",
    "###### `E.pkl`\n",
    "\n",
    "This file contains the class object itself and a collection of its attributes. It, importantly, _does not_\n",
    "contain the nested dataframes (`subjects_df`, `events_df`, `dynamic_measurements_df`), as these are stored in\n",
    "the files mentioned above and loaded lazily by the object during use, and similarly does not store the learned\n",
    "measurement metadata files also mentioned above, which are lazily loaded in the same way.\n",
    "\n",
    "```{warning}\n",
    "Currently, this lazy saving/loading uses absolute paths, which makes transferring datasets to new locations\n",
    "more challenging! Paths can be modified in the class and config object locally to fix this problem, but it is\n",
    "a challenge.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e500e9f",
   "metadata": {},
   "source": [
    "## Inspecting the Synthetic Dataset Produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78458ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "\n",
    "pl.Config.set_tbl_cols(7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"processed/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25b21b",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6735b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESD.subjects_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee61e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESD.events_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESD.dynamic_measurements_df.filter(pl.col(\"event_id\") == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9a8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
